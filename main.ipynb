{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1422dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6badb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stream_chat_bot:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.llm = Llama(model_path=self.model_path, n_ctx=16384, n_gpu_layers=99,verbose=False)\n",
    "        self.clean()\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "\n",
    "    def answer(self, query):\n",
    "        self.push_back(\"user\", query)\n",
    "        output = self.llm(self.stream + \"<|start_header_id|>assistant<|end_header_id|>\", max_tokens=2048)\n",
    "        self.push_back(\"assistant\", f\"{output['choices'][0]['text']}\")\n",
    "        return output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d88f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = stream_chat_bot(\"llama-3.1-8b-instruct-q4_k_m.gguf\")\n",
    "while True:\n",
    "    print(bot.answer(input(\"\\n>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bf04a",
   "metadata": {},
   "source": [
    "# sentence as node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "\n",
    "    class MCTS_NODE:\n",
    "        def __init__(self,parent,actions_and_probs,k):\n",
    "            self.parent = parent\n",
    "            self.last_visit = -1\n",
    "            self.end = False\n",
    "            (self.actions, self.probs) = actions_and_probs\n",
    "            self.actions = np.array(self.actions)\n",
    "            self.probs = np.array(self.probs)\n",
    "            self.child = np.full((k,),-1)\n",
    "            self.mean_reward = np.full((k,),0)\n",
    "            self.visit_count = 0\n",
    "            self.child_visit_count = np.full((k,),0)\n",
    "\n",
    "        def get_optimum_child(self, policy_weight, explore_weight):\n",
    "            score = self.mean_reward + self.probs*policy_weight + \\\n",
    "                (np.sqrt(np.log(self.visit_count + 1e-6))/(self.child_visit_count + 1e-6))*explore_weight\n",
    "            return self.child[np.argmax(score)]\n",
    "\n",
    "    def __init__(self, model_path, k, policy_weight = 1, explore_weight = 1):\n",
    "        self.model = Llama(model_path, n_ctx = 16384, n_gpu_layers = 99,logits_all = True,verbose = False)\n",
    "        self.clean()\n",
    "        self.nodes = []\n",
    "        self.policy_weight = policy_weight\n",
    "        self.explore_weight = explore_weight   \n",
    "        self.k = k\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "    \n",
    "    def generate_action_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 256,\n",
    "                                      logprobs = 0, temperature = 0, stop='\\n')[\"choices\"][0][\"text\"] + \"\\n\")\n",
    "        return (actions, probs)\n",
    "    \n",
    "    def generate_answer_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 512,\n",
    "                                      logprobs = 0, temperature = 0)[\"choices\"][0][\"text\"])\n",
    "        return (actions, probs)\n",
    "    \n",
    "    # given a query and an answer, evaluate the answer\n",
    "    # this function is a placeholder and should be implemented based on the specific evaluation criteria\n",
    "    def answer_evaluate(self, query_answer):\n",
    "        pass\n",
    "    \n",
    "    def MCTS_initialize(self):\n",
    "        self.nodes = []\n",
    "        self.deleted = []\n",
    "        self.nodes.append(MCTS_REASONING_LLM.MCTS_NODE(-1, self.generate_action_list(self.stream), self.k))\n",
    "        self.set_root(0)\n",
    "\n",
    "    def new_node(self, parent, actions_and_probs):\n",
    "        new_node = MCTS_REASONING_LLM.MCTS_NODE(parent, actions_and_probs, self.k)\n",
    "        if(len(self.deleted)):\n",
    "            self.nodes[self.deleted[0]] = new_node\n",
    "            idx = self.deleted[0]\n",
    "            self.deleted = self.deleted[1:]\n",
    "            return idx \n",
    "        else:\n",
    "            self.nodes.append(new_node)\n",
    "            return len(self.nodes) - 1\n",
    "    \n",
    "    def delete_node(self, idx):\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def delete_tree(self, idx):\n",
    "        for child in self.nodes[idx].child:\n",
    "            if(child != -1):\n",
    "                self.delete_tree(child)\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def set_root(self,idx):\n",
    "        self.root = idx\n",
    "        self.nodes[idx].parent = -1\n",
    "    \n",
    "    def select_and_expand(self):\n",
    "        previous_node = -1\n",
    "        current_node = self.root\n",
    "        last_visit = -1\n",
    "        current_prompt = self.stream\n",
    "        while current_node != -1 and self.nodes[current_node].end == False:\n",
    "            last_visit = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = last_visit\n",
    "            current_prompt += self.nodes[current_node].actions[last_visit]\n",
    "            previous_node = current_node\n",
    "            current_node = self.nodes[current_node].child[last_visit]\n",
    "\n",
    "        if current_node != -1:\n",
    "            return current_node, current_prompt\n",
    "        else:\n",
    "            if self.nodes[previous_node].actions[last_visit] == \"[answer]\\n\":\n",
    "                actions, probs = self.generate_answer_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "                self.nodes[current_node].end = True\n",
    "                for i in range(len(self.nodes[current_node].child)):\n",
    "                    self.nodes[current_node].mean_reward[i] = self.answer_evaluate(self.stream + self.nodes[current_node].actions[i] + '<|eot_id|>')\n",
    "            else:\n",
    "                actions, probs = self.generate_action_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "            return current_node, current_prompt\n",
    "        \n",
    "    def simulation(self, current_node, current_prompt):\n",
    "        if self.nodes[current_node].end:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reward = self.nodes[current_node].mean_reward[optimum_child]\n",
    "            return reward\n",
    "        else:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reasoning = self.nodes[current_node].actions[optimum_child]\n",
    "            while(reasoning != \"[answer]\\n\"):\n",
    "                current_prompt += reasoning\n",
    "                reasoning = self.model(prompt = current_prompt, max_tokens = 256, temperature = 0, stop = '\\n', logprobs = 0)[\"choices\"][0][\"text\"] + \"\\n\"\n",
    "            current_prompt += reasoning\n",
    "            respond = self.model(prompt = current_prompt, max_tokens = 512, temperature = 0,  logprobs = 0)[\"choices\"][0][\"text\"] + \"<|eot_id|>\"\n",
    "            return self.answer_evaluate(self.stream + respond + '<|eot_id|>')\n",
    "            \n",
    "    def backpropagation(self, current_node, reward):\n",
    "        while current_node != -1:\n",
    "            self.nodes[current_node].visit_count += 1\n",
    "            self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit] += 1\n",
    "            self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit] += \\\n",
    "            (reward - self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit]) / self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit]\n",
    "            current_node = self.nodes[current_node].parent\n",
    "\n",
    "    def query(self,query,iterations=100):\n",
    "        self.push_back(\"user\",query)\n",
    "        self.stream += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.MCTS_initialize()\n",
    "        while(not self.nodes[self.root].end):\n",
    "            while self.nodes[self.root].visit_count < iterations :\n",
    "                current_node, current_prompt = self.select_and_expand()\n",
    "                reward = self.simulation(current_node, current_prompt)\n",
    "                self.backpropagation(current_node, reward)\n",
    "            optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "            self.stream += self.nodes[self.root].actions[optimum_child]\n",
    "            for i in range(self.k):\n",
    "                if i != optimum_child and i != -1:\n",
    "                    self.delete_tree(self.nodes[self.root].child[i])\n",
    "            new_root = self.nodes[self.root].child[optimum_child]\n",
    "            self.delete_node(self.root)\n",
    "            self.set_root(new_root)\n",
    "        optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "        respond = self.nodes[self.root].actions[optimum_child]\n",
    "        self.stream += respond + '<|eot_id|>'\n",
    "        return respond\n",
    "            \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b8120",
   "metadata": {},
   "source": [
    "# paragraph as node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] Starting MCTS with 20 iterations...\n",
      "[mcts_init] Initializing MCTS for query: What is the sum of the first 10 prime numbers?...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The answer is an outright refusal to help, which is unacceptable in an academic or prob...\n",
      "[self_evaluate] Sample 1 extracted score: -100\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The answer is an outright refusal to help, which is unacceptable in an academic or prob...\n",
      "[self_evaluate] Sample 1 extracted score: -100\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The answer provided is a single sentence stating that they cannot help with the questio...\n",
      "[self_evaluate] Sample 2 extracted score: -100\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The answer provided is a single sentence stating that they cannot help with the questio...\n",
      "[self_evaluate] Sample 2 extracted score: -100\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is an obvious refusal to attempt the problem, providing no explanation or ...\n",
      "[self_evaluate] Sample 3 extracted score: -95\n",
      "[self_evaluate] Final scores: [-100, -100, -95]\n",
      "[mcts_init] Root node created with Q-value: -99.16666666666666\n",
      "[run] Iteration 1/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is an obvious refusal to attempt the problem, providing no explanation or ...\n",
      "[self_evaluate] Sample 3 extracted score: -95\n",
      "[self_evaluate] Final scores: [-100, -100, -95]\n",
      "[mcts_init] Root node created with Q-value: -99.16666666666666\n",
      "[run] Iteration 1/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: ### Step 1: Understand what prime numbers are.\n",
      "- **Good**: The answer correctly starts by defining p...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: ### Step 1: Understand what prime numbers are.\n",
      "- **Good**: The answer correctly starts by defining p...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is correct in identifying the first 10 prime numbers and the process to su...\n",
      "[self_evaluate] Sample 1 extracted score: 25\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is correct in identifying the first 10 prime numbers and the process to su...\n",
      "[self_evaluate] Sample 1 extracted score: 25\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is straightforward and correctly identifies the first 10 prime numbers. Ho...\n",
      "[self_evaluate] Sample 2 extracted score: 80\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is straightforward and correctly identifies the first 10 prime numbers. Ho...\n",
      "[self_evaluate] Sample 2 extracted score: 80\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear step-by-step process and justifica...\n",
      "[self_evaluate] Sample 3 extracted score: 35\n",
      "[self_evaluate] Final scores: [25, 80, 35]\n",
      "[run] Iteration 2/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear step-by-step process and justifica...\n",
      "[self_evaluate] Sample 3 extracted score: 35\n",
      "[self_evaluate] Final scores: [25, 80, 35]\n",
      "[run] Iteration 2/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] The sum of the first 10 prime numbers can be found by listing out the first 10 p...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] The sum of the first 10 prime numbers can be found by listing out the first 10 p...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: First, the student is correct that they are looking for the sum of the first 10 prime numbers. \n",
      "\n",
      "How...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: First, the student is correct that they are looking for the sum of the first 10 prime numbers. \n",
      "\n",
      "How...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it has some flaws. Firstly, the explanation of prim...\n",
      "[self_evaluate] Sample 1 extracted score: -30\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it has some flaws. Firstly, the explanation of prim...\n",
      "[self_evaluate] Sample 1 extracted score: -30\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but there are some minor flaws. The reasoning process i...\n",
      "[self_evaluate] Sample 2 extracted score: -20\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but there are some minor flaws. The reasoning process i...\n",
      "[self_evaluate] Sample 2 extracted score: -20\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear and explicit reasoning process. Th...\n",
      "[self_evaluate] Sample 3 extracted score: -50\n",
      "[self_evaluate] Final scores: [-30, -20, -50]\n",
      "[run] Iteration 3/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear and explicit reasoning process. Th...\n",
      "[self_evaluate] Sample 3 extracted score: -50\n",
      "[self_evaluate] Final scores: [-30, -20, -50]\n",
      "[run] Iteration 3/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] First, we need to identify what prime numbers are. Prime numbers are numbers gre...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] First, we need to identify what prime numbers are. Prime numbers are numbers gre...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: The answer provided is a good attempt, but there are a few areas where it could be improved. Here's ...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: The answer provided is a good attempt, but there are a few areas where it could be improved. Here's ...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks proper reasoning and has minor flaws. The ...\n",
      "[self_evaluate] Sample 1 extracted score: -20\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks proper reasoning and has minor flaws. The ...\n",
      "[self_evaluate] Sample 1 extracted score: -20\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct in identifying prime numbers and listing out the first 1...\n",
      "[self_evaluate] Sample 2 extracted score: -60\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct in identifying prime numbers and listing out the first 1...\n",
      "[self_evaluate] Sample 2 extracted score: -60\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly clear and easy to follow, but it lacks a thorough and rigorous a...\n",
      "[self_evaluate] Sample 3 extracted score: -70\n",
      "[self_evaluate] Final scores: [-20, -60, -70]\n",
      "[run] Iteration 4/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly clear and easy to follow, but it lacks a thorough and rigorous a...\n",
      "[self_evaluate] Sample 3 extracted score: -70\n",
      "[self_evaluate] Final scores: [-20, -60, -70]\n",
      "[run] Iteration 4/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: First, we need to understand that prime numbers are numbers greater than 1 that have no divisors oth...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: First, we need to understand that prime numbers are numbers greater than 1 that have no divisors oth...\n",
      "[generate_critique] Generating critique for solution...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MCTS_NODE:\n",
    "    def __init__(self, parent, solution, critique, Q_value):\n",
    "        self.parent = parent\n",
    "        self.solution = solution\n",
    "        self.critique = critique\n",
    "        self.Q_value = Q_value\n",
    "        self.visit_count = 0\n",
    "        self.children = []  \n",
    "        self.reward_samples = [] \n",
    "        self.fully_expanded = False \n",
    "        self.expanded_children = 0 \n",
    "    \n",
    "class MCTS_REASONING_LLM:\n",
    "    def __init__(self, model_path, max_child=5, c=1):\n",
    "        self.model = Llama(model_path, n_ctx=16384, n_gpu_layers=99, logits_all=False, verbose=False)\n",
    "        self.max_child = max_child\n",
    "        self.c = c\n",
    "        self.nodes = []\n",
    "        self.query = None\n",
    "        self.dummy_answers = [\n",
    "            \"I Don't Know\",\n",
    "            \"I can't understand this question.\",\n",
    "            \"I can't help with this question.\",\n",
    "            \"I don't know how to solve this question.\",\n",
    "            \"I don't know the answer to this question.\",\n",
    "            \"I don't know the answer to this question, sorry.\"\n",
    "        ]\n",
    "\n",
    "    def is_fully_expanded(self, idx): \n",
    "        if len(self.nodes[idx].children) >= self.max_child:\n",
    "            return True\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            if self.nodes[child_idx].Q_value > self.nodes[idx].Q_value:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_optimum_child(self, idx):\n",
    "        if not self.nodes[idx].children: \n",
    "            return -1\n",
    "        if self.nodes[idx].parent != -1:\n",
    "            parent_visit_count = self.nodes[self.nodes[idx].parent].visit_count\n",
    "        else:\n",
    "            parent_visit_count = 1\n",
    "        UCT = []\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            child_visit_count = self.nodes[child_idx].visit_count\n",
    "            uct_value = (self.nodes[child_idx].Q_value + \n",
    "                        self.c * math.sqrt(math.log(parent_visit_count + 1) / (child_visit_count + 1e-6)))\n",
    "            UCT.append(uct_value)\n",
    "        \n",
    "        if (not self.nodes[idx].fully_expanded and \n",
    "            np.max(UCT) < self.c * math.sqrt(math.log(parent_visit_count + 1) / 1e-6)):\n",
    "            return -1\n",
    "            \n",
    "        return self.nodes[idx].children[np.argmax(UCT)]\n",
    "\n",
    "    def generate_critique(self, query, solution):\n",
    "        print(f\"[generate_critique] Generating critique for solution...\")\n",
    "        \n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Since we have a weak Answer, could you provide me with a reflection or feedback to correct this answer better? Analyze this Answer Strictly and Critically, point out every flaw for every possible imperfect to minus every possible score!\n",
    "\n",
    "Question: {query}\n",
    "Answer: {solution}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=1024, temperature=0.8)\n",
    "            critique = response[\"choices\"][0][\"text\"].strip()\n",
    "            print(f\"[generate_critique] Critique generated successfully: {critique[:100]}...\")\n",
    "            return critique\n",
    "        except Exception as e:\n",
    "            print(f\"[generate_critique] ERROR: {str(e)}\")\n",
    "            return \"The answer needs improvement.\"\n",
    "\n",
    "    def generate_refined_solution(self, query, original_solution, critique):\n",
    "        print(f\"[generate_refined_solution] Refining solution...\")\n",
    "        \n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Please refine your answer according to the Reflection or Feedback. The response should begin with [reasoning process]...[Verification]... and end with \"[Final Answer] The answer is [answer formula]\"\n",
    "\n",
    "Question: {query}\n",
    "Original Answer: {original_solution}\n",
    "Feedback: {critique}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=2048, temperature=0.8)\n",
    "            refined = response[\"choices\"][0][\"text\"].strip()\n",
    "            print(f\"[generate_refined_solution] Refined solution generated successfully: {refined[:100]}...\")\n",
    "            return refined\n",
    "        except Exception as e:\n",
    "            print(f\"[generate_refined_solution] ERROR: {str(e)}\")\n",
    "            return original_solution\n",
    "\n",
    "    def extract_score_from_text(self, text):\n",
    "        # Look for patterns like [Score] -50, [Score]: -50, Score: -50, etc.\n",
    "        score_patterns = [\n",
    "            r'\\[Score\\]\\s*[-]?\\d+',  # [Score] -50\n",
    "            r'\\[Score\\]:\\s*[-]?\\d+',  # [Score]: -50\n",
    "            r'Score:\\s*[-]?\\d+',     # Score: -50\n",
    "            r'Score\\s+[-]?\\d+',      # Score -50\n",
    "            r'score\\s*[:=]\\s*[-]?\\d+',  # score: -50 or score = -50\n",
    "        ]\n",
    "        \n",
    "        for pattern in score_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                # Extract just the number from the match\n",
    "                number_match = re.search(r'[-]?\\d+', match.group())\n",
    "                if number_match:\n",
    "                    return int(number_match.group())\n",
    "        \n",
    "        # Fallback: look for the last number in the text (often the final score)\n",
    "        all_numbers = re.findall(r'[-]?\\d+', text)\n",
    "        if all_numbers:\n",
    "            # Filter numbers to reasonable score range\n",
    "            valid_scores = [int(num) for num in all_numbers if -100 <= int(num) <= 100]\n",
    "            if valid_scores:\n",
    "                return valid_scores[-1]  # Take the last valid score\n",
    "        \n",
    "        return 0  # Default if no score found\n",
    "\n",
    "    def self_evaluate(self, query, solution, num_samples=3):\n",
    "        print(f\"[self_evaluate] Evaluating solution with {num_samples} samples...\")\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(num_samples):\n",
    "            prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {query}\n",
    "Answer: {solution}\n",
    "\n",
    "Analyze this Answer Strictly and Critically, and point out every flaw for every possible imperfect to minus every possible score! You need to be very harsh and mean in calculating grades, and never give full marks to ensure that the marks are authoritative.\n",
    "\n",
    "Output a score between [-100,+100].\n",
    "\n",
    "Format: [Analysis] your analysis here [Score] your_number_here\n",
    "\n",
    "Example: [Analysis] The solution has calculation errors and lacks proper reasoning. [Score] -45<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.model(prompt=prompt, max_tokens=512, temperature=0.8)\n",
    "                text = response[\"choices\"][0][\"text\"]\n",
    "                print(f\"[self_evaluate] Sample {i+1} response: {text[:100]}...\")\n",
    "                \n",
    "                # Extract score using improved method\n",
    "                score = self.extract_score_from_text(text)\n",
    "                \n",
    "                # Full Score Suppression: reduce scores above 95\n",
    "                if score > 95:\n",
    "                    score = max(95, score - 10)\n",
    "                \n",
    "                # Clamp score to valid range\n",
    "                score = max(-100, min(100, score))\n",
    "                scores.append(score)\n",
    "                print(f\"[self_evaluate] Sample {i+1} extracted score: {score}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[self_evaluate] ERROR in sample {i+1}: {str(e)}\")\n",
    "                scores.append(0)\n",
    "        \n",
    "        print(f\"[self_evaluate] Final scores: {scores}\")\n",
    "        return scores\n",
    "    \n",
    "    def calculate_q_value(self, reward_samples):\n",
    "        \"\"\"Calculate Q value using formula from paper: Q(a) = 1/2 * (min(R_a) + mean(R_a))\"\"\"\n",
    "        if not reward_samples:\n",
    "            return 0\n",
    "        \n",
    "        min_reward = min(reward_samples)\n",
    "        mean_reward = sum(reward_samples) / len(reward_samples)\n",
    "        q_value = 0.5 * (min_reward + mean_reward)\n",
    "        \n",
    "        return q_value\n",
    "    \n",
    "    def update_q_value_with_children(self, node_index):\n",
    "        \"\"\"Update Q value considering children: Q'(a) = 1/2 * (Q(a) + max_child_Q)\"\"\"\n",
    "        node = self.nodes[node_index]\n",
    "        \n",
    "        # Calculate base Q value from own rewards\n",
    "        base_q = self.calculate_q_value(node.reward_samples)\n",
    "        \n",
    "        # Find maximum Q value among children\n",
    "        max_child_q = float('-inf')\n",
    "        has_children = False\n",
    "        \n",
    "        for child_idx in node.children:\n",
    "            child_q = self.nodes[child_idx].Q_value\n",
    "            max_child_q = max(max_child_q, child_q)\n",
    "            has_children = True\n",
    "        \n",
    "        # Update Q value: Q'(a) = 1/2 * (Q(a) + max_child_Q)\n",
    "        if has_children:\n",
    "            node.Q_value = 0.5 * (base_q + max_child_q)\n",
    "        else:\n",
    "            node.Q_value = base_q\n",
    "\n",
    "    def mcts_init(self, query):\n",
    "        print(f\"[mcts_init] Initializing MCTS for query: {query[:50]}...\")\n",
    "        \n",
    "        self.query = query\n",
    "        self.nodes = []\n",
    "        \n",
    "        # Create root node with dummy answer\n",
    "        dummy_solution = random.choice(self.dummy_answers)\n",
    "        critique = self.generate_critique(self.query, dummy_solution)\n",
    "        \n",
    "        # Create root node\n",
    "        root_node = MCTS_NODE(-1, dummy_solution, critique, 0)\n",
    "        \n",
    "        # Evaluate root node\n",
    "        root_node.reward_samples = self.self_evaluate(query, dummy_solution)\n",
    "        root_node.Q_value = self.calculate_q_value(root_node.reward_samples)\n",
    "        root_node.visit_count = 1\n",
    "        \n",
    "        self.nodes.append(root_node)\n",
    "        print(f\"[mcts_init] Root node created with Q-value: {root_node.Q_value}\")\n",
    "\n",
    "    def iterator(self):\n",
    "        \"\"\"Single MCTS iteration combining all phases: Selection -> Expansion -> Evaluation -> Backpropagation\"\"\"\n",
    "        # SELECTION PHASE: Navigate to leaf node\n",
    "        current_node = 0\n",
    "        previous_node = -1\n",
    "        while current_node != -1:\n",
    "            previous_node = current_node\n",
    "            current_node = self.get_optimum_child(current_node)\n",
    "        \n",
    "        # EXPANSION PHASE: Create refined solution\n",
    "        solution = self.generate_refined_solution(\n",
    "            self.query, \n",
    "            self.nodes[previous_node].solution, \n",
    "            self.nodes[previous_node].critique \n",
    "        )\n",
    "        critique = self.generate_critique(self.query, solution)\n",
    "        \n",
    "        # Create new child node\n",
    "        new_node = MCTS_NODE(previous_node, solution, critique, 0)\n",
    "        self.nodes.append(new_node)\n",
    "        current_node = len(self.nodes) - 1\n",
    "        \n",
    "        # Add child to parent's children list\n",
    "        self.nodes[previous_node].children.append(current_node)\n",
    "        \n",
    "        # EVALUATION PHASE: Self-evaluate the new solution\n",
    "        self.nodes[current_node].reward_samples = self.self_evaluate(self.query, solution)\n",
    "        self.nodes[current_node].Q_value = self.calculate_q_value(self.nodes[current_node].reward_samples)\n",
    "        self.nodes[current_node].visit_count = 1\n",
    "        \n",
    "        # BACKPROPAGATION PHASE: Update Q values up the tree\n",
    "        while previous_node != -1:\n",
    "            self.nodes[previous_node].visit_count += 1\n",
    "            self.update_q_value_with_children(previous_node)\n",
    "            self.nodes[previous_node].fully_expanded = self.is_fully_expanded(previous_node)  # Fixed: was 'fully_expended'\n",
    "            previous_node = self.nodes[previous_node].parent\n",
    "\n",
    "    def run(self, query, iterations=100):\n",
    "        \"\"\"Run MCTS for specified number of iterations\"\"\"\n",
    "        print(f\"[run] Starting MCTS with {iterations} iterations...\")\n",
    "        self.mcts_init(query)\n",
    "        for i in range(iterations):\n",
    "            print(f\"[run] Iteration {i+1}/{iterations}\")\n",
    "            self.iterator()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "                print(f\"[run] Best Q-value after {i+1} iterations: {best_node.Q_value}\")\n",
    "        \n",
    "        # Return best solution\n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        print(f\"[run] Final best Q-value: {best_node.Q_value}\")\n",
    "        return best_node.solution\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"Get the solution with highest Q-value\"\"\"\n",
    "        if not self.nodes:\n",
    "            return None\n",
    "        \n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        return best_node.solution\n",
    "    \n",
    "    def print_status(self):\n",
    "        print(\"printing MCTS status\")\n",
    "        idx = 0\n",
    "        for nodes in self.nodes:\n",
    "            print(f\"node_{idx}:\\n\"\n",
    "                  f\"parent: {nodes.parent}, children: {nodes.children}\\n\"\n",
    "                  f\"solution:\\n\"\n",
    "                  f\"{nodes.solution}\\n\"\n",
    "                  \"critique:\\n\"\n",
    "                  f\"{nodes.critique}\\n\"\n",
    "                  f\"Q_value: {nodes.Q_value}\\n\"\n",
    "                  f\"evaluate_samples:\\n\"\n",
    "                  f\"{nodes.reward_samples}\\n\")\n",
    "            idx += 1\n",
    "    \n",
    "\n",
    "# Initialize the model\n",
    "mctsr = MCTS_REASONING_LLM(\"llama-3.1-8b-instruct-q4_k_m.gguf\")\n",
    "\n",
    "# Solve a problem\n",
    "query = \"What is the sum of the first 10 prime numbers?\"\n",
    "solution = mctsr.run(query, 20)\n",
    "\n",
    "# Get tree statistics\n",
    "stats = mctsr.get_tree_stats()\n",
    "print(f\"Generated {stats['total_nodes']} nodes with max Q-value: {stats['max_q_value']}\")\n",
    "mctsr.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6971b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing MCTS status\n",
      "node_0:\n",
      "parent: -1, children: [1]\n",
      "solution:\n",
      "I Don't Know\n",
      "critique:\n",
      "What is the problem with the answer \"I Don't Know\"? \n",
      "\n",
      "1.  **Lack of Effort**: The answer \"I Don't Know\" indicates a lack of effort or attempt to solve the riddle. It shows a deficiency in critical thinking and problem-solving skills.\n",
      "2.  **Lack of Analysis**: The answer doesn't show any analysis of the clues provided in the riddle. It doesn't try to understand the meaning behind each line or think about possible connections between them.\n",
      "3.  **Insufficient Information**: The answer doesn't take into account all the information given in the riddle. It doesn't consider the possible connections between the color, size, position, presence in sun, absence in rain, and the fact that it doesn't harm anyone and feels no pain.\n",
      "4.  **Lack of Creativity**: The answer is too generic and doesn't show any creative thinking. It's a quick and lazy response that doesn't attempt to find a solution.\n",
      "\n",
      "Overall, I would give the answer \"I Don't Know\" a score of 0 out of 10. It doesn't demonstrate any of the skills required to solve the riddle, such as critical thinking, analysis, and creativity.\n",
      "Q_value: -76.39583333333333\n",
      "evaluate_samples:\n",
      "[-85, -100, -98]\n",
      "\n",
      "node_1:\n",
      "parent: 0, children: [2]\n",
      "solution:\n",
      "[Reasoning Process]\n",
      "To solve this riddle, we need to analyze each line carefully and think about the possible connections between them. We should try to understand the meaning behind each clue and think about how they relate to each other. \n",
      "\n",
      "[Analysis]\n",
      "Let's break down the riddle and analyze each line:\n",
      "\n",
      "*   \"Only one color, but not one size\": This line suggests that the object has only one color, but it can come in different sizes. This means that the object is likely something that can be manufactured or produced in different sizes, but its color remains the same.\n",
      "*   \"Stuck at the bottom, yet easily flies\": This line is a bit contradictory, but it suggests that the object is stuck or attached to something at the bottom, but it can still fly or move easily. This means that the object is likely something that can be attached to a surface or a structure, but it can still move or fly.\n",
      "*   \"Present in sun, but not in rain\": This line suggests that the object is present or visible in sunny weather, but it disappears or is not visible in rainy weather. This means that the object is likely something that is sensitive to weather conditions or that is affected by the presence or absence of sunlight.\n",
      "*   \"Doing no harm, and feeling no pain\": This line suggests that the object is harmless and doesn't feel any pain or discomfort. This means that the object is likely something that is inanimate or doesn't have feelings or emotions.\n",
      "\n",
      "[Verification]\n",
      "Based on the analysis above, let's think about possible objects that fit the description. One possible object that comes to mind is a flag. Flags are typically made of a single color, but they can come in different sizes. They are often stuck or attached to a flagpole at the bottom, but they can still fly or move easily in the wind. Flags are present in sunny weather, but they may not be visible in rainy weather. Finally, flags are inanimate objects that don't feel any pain or discomfort.\n",
      "\n",
      "[Final Answer] The answer is a flag.\n",
      "critique:\n",
      "This answer is weak for several reasons:\n",
      "\n",
      "1.  **Lack of clear analysis**: The analysis is superficial and doesn't provide a clear and concise explanation of the answer. It doesn't break down the clues and connect them in a logical way.\n",
      "2.  **Insufficient evidence**: The answer relies on assumptions and doesn't provide enough evidence to support the conclusion. It assumes that the object is a flag without providing a clear connection between the clues and the answer.\n",
      "3.  **Lack of creativity**: The answer is too straightforward and doesn't provide any creative or innovative solutions. It relies on a common and obvious answer without exploring other possibilities.\n",
      "4.  **Poor structure**: The answer is disorganized and doesn't follow a logical structure. It jumps between ideas and doesn't provide a clear conclusion.\n",
      "5.  **Lack of critical thinking**: The answer doesn't demonstrate critical thinking skills. It doesn't evaluate the clues critically and doesn't consider alternative explanations.\n",
      "\n",
      "To improve this answer, you could:\n",
      "\n",
      "1.  **Provide a clear and concise analysis**: Break down the clues and connect them in a logical way to arrive at the answer.\n",
      "2.  **Provide sufficient evidence**: Support the conclusion with evidence from the clues and demonstrate a clear connection between the clues and the answer.\n",
      "3.  **Be creative**: Explore alternative possibilities and provide innovative solutions to the riddle.\n",
      "4.  **Organize the answer**: Structure the answer in a logical and coherent way to make it easier to follow.\n",
      "5.  **Demonstrate critical thinking**: Evaluate the clues critically and consider alternative explanations to arrive at the answer.\n",
      "\n",
      "Overall, this answer needs significant improvement to provide a clear, concise, and creative solution to the riddle.\n",
      "Q_value: -55.625\n",
      "evaluate_samples:\n",
      "[-70, -20, -60]\n",
      "\n",
      "node_2:\n",
      "parent: 1, children: [3, 4, 5]\n",
      "solution:\n",
      "[Reasoning Process]\n",
      "To solve this riddle, we need to analyze each line carefully and think about the possible connections between them. We should try to understand the meaning behind each clue and think about how they relate to each other.\n",
      "\n",
      "[Analysis]\n",
      "Let's break down the riddle and analyze each line:\n",
      "\n",
      "*   \"Only one color, but not one size\": This line suggests that the object has only one color, but it can come in different sizes. This means that the object is likely something that can be manufactured or produced in different sizes, but its color remains the same.\n",
      "*   \"Stuck at the bottom, yet easily flies\": This line is a bit contradictory, but it suggests that the object is stuck or attached to something at the bottom, but it can still fly or move easily. This means that the object is likely something that can be attached to a surface or a structure, but it can still move or fly.\n",
      "*   \"Present in sun, but not in rain\": This line suggests that the object is present or visible in sunny weather, but it disappears or is not visible in rainy weather. This means that the object is likely something that is sensitive to weather conditions or that is affected by the presence or absence of sunlight.\n",
      "*   \"Doing no harm, and feeling no pain\": This line suggests that the object is harmless and doesn't feel any pain or discomfort. This means that the object is likely something that is inanimate or doesn't have feelings or emotions.\n",
      "\n",
      "[Verification]\n",
      "Based on the analysis above, let's think about possible objects that fit the description. A possible object that comes to mind is a kite. Kites are typically made of a single color, but they can come in different sizes. They are often stuck or attached to a string at the bottom, but they can still fly or move easily in the wind. Kites are present in sunny weather, but they may not be visible in rainy weather. Finally, kites are inanimate objects that don't feel any pain or discomfort.\n",
      "\n",
      "[Final Answer] The answer is a kite.\n",
      "critique:\n",
      "You provided a very clear and concise solution, but to provide feedback, we can highlight some areas for improvement. Here are some suggestions to enhance the answer:\n",
      "\n",
      "1.  **More detailed analysis**: While the analysis you provided is good, it could be more detailed and in-depth. For example, you could explore the meaning of each line more thoroughly and provide more concrete examples or connections between the clues.\n",
      "2.  **Clearer reasoning process**: The reasoning process you provided is mostly in the form of analysis, but it could be more clearly structured as a step-by-step process. Consider breaking down the solution into more distinct steps and using transitional phrases to connect them.\n",
      "3.  **More attention to grammar and punctuation**: There are a few places where the grammar and punctuation could be improved. For example, some sentences could be rephrased for better clarity, and there are a few missing commas.\n",
      "4.  **More attention to formatting**: The text could benefit from more consistent formatting and indentation. Consider using headings, bullet points, or numbered lists to break up the text and make it easier to read.\n",
      "5.  **More consideration of alternative solutions**: While the solution you provided is a good one, it's always a good idea to consider alternative solutions and explain why they don't fit as well. This can help strengthen the solution and make it more convincing.\n",
      "\n",
      "Here's an updated version of the solution incorporating these suggestions:\n",
      "\n",
      "**Step 1: Analyze the first line**\n",
      "\n",
      "The first line states, \"Only one color, but not one size.\" This suggests that the object has only one color, but it can come in different sizes. This means that the object is likely something that can be manufactured or produced in different sizes, but its color remains the same.\n",
      "\n",
      "**Step 2: Analyze the second line**\n",
      "\n",
      "The second line states, \"Stuck at the bottom, yet easily flies.\" This line is a bit contradictory, but it suggests that the object is stuck or attached to something at the bottom, but it can still fly or move easily. This means that the object is likely something that can be attached to a surface or a structure, but it can still move or fly.\n",
      "\n",
      "**Step 3: Analyze the third line**\n",
      "\n",
      "The third line states, \"Present in sun, but not in rain.\" This line suggests that the object is present or visible in sunny weather, but it disappears or is not visible in rainy weather. This means that the object is likely something that is sensitive to weather conditions or that is affected by the presence or absence of sunlight.\n",
      "\n",
      "**Step 4: Analyze the fourth line**\n",
      "\n",
      "The fourth line states, \"Doing no harm, and feeling no pain.\" This line suggests that the object is harmless and doesn't feel any pain or discomfort. This means that the object is likely something that is inanimate or doesn't have feelings or emotions.\n",
      "\n",
      "**Step 5: Combine the clues**\n",
      "\n",
      "Based on the analysis above, let's think about possible objects that fit the description. A possible object that comes to mind is a kite. Kites are typically made of a single color, but they can come in different sizes. They are often stuck or attached to a string at the bottom, but they can still fly or move easily in the wind. Kites are present in sunny weather, but they may not be visible in rainy weather. Finally, kites are inanimate objects that don't feel any pain or discomfort.\n",
      "\n",
      "**Final Answer**\n",
      "\n",
      "The answer is a kite.\n",
      "Q_value: -51.25\n",
      "evaluate_samples:\n",
      "[80, -75, -60]\n",
      "\n",
      "node_3:\n",
      "parent: 2, children: []\n",
      "solution:\n",
      "[Reasoning Process]\n",
      "To solve this riddle, we need to analyze each line carefully and think about the possible connections between them. We should try to understand the meaning behind each clue and think about how they relate to each other.\n",
      "\n",
      "[Analysis]\n",
      "Let's break down the riddle and analyze each line:\n",
      "\n",
      "*   \"Only one color, but not one size\": This line suggests that the object has only one color, but it can come in different sizes. This means that the object is likely something that can be manufactured or produced in different sizes, but its color remains the same.\n",
      "*   \"Stuck at the bottom, yet easily flies\": This line is a bit contradictory, but it suggests that the object is stuck or attached to something at the bottom, but it can still fly or move easily. This means that the object is likely something that can be attached to a surface or a structure, but it can still move or fly.\n",
      "*   \"Present in sun, but not in rain\": This line suggests that the object is present or visible in sunny weather, but it disappears or is not visible in rainy weather. This means that the object is likely something that is sensitive to weather conditions or that is affected by the presence or absence of sunlight.\n",
      "*   \"Doing no harm, and feeling no pain\": This line suggests that the object is harmless and doesn't feel any pain or discomfort. This means that the object is likely something that is inanimate or doesn't have feelings or emotions.\n",
      "\n",
      "[Verification]\n",
      "Based on the analysis above, let's think about possible objects that fit the description. A possible object that comes to mind is a kite. Kites are typically made of a single color, but they can come in different sizes. They are often stuck or attached to a string at the bottom, but they can still fly or move easily in the wind. Kites are present in sunny weather, but they may not be visible in rainy weather. Finally, kites are inanimate objects that don't feel any pain or discomfort.\n",
      "\n",
      "[Final Answer] The answer is a kite.\n",
      "critique:\n",
      "The given answer is a kite. Let's verify if the given answer is correct or not.\n",
      "\n",
      "**Verification of the Given Answer**\n",
      "\n",
      "1.  **Only one color, but not one size**:\n",
      "    *   The given answer is a kite. Kites are available in different colors, but they come in a variety of sizes as well.\n",
      "    *   **Score: 7/10** (The answer is partially correct, but not entirely accurate.)\n",
      "2.  **Stuck at the bottom, yet easily flies**:\n",
      "    *   The given answer is a kite. A kite is indeed stuck to the ground or the string at the bottom, yet it easily flies in the air when the wind blows.\n",
      "    *   **Score: 9/10** (The answer is mostly correct, but there might be some room for improvement in the explanation.)\n",
      "3.  **Present in sun, but not in rain**:\n",
      "    *   The given answer is a kite. Kites are often flown in sunny weather, but they are less likely to be flown in rainy weather.\n",
      "    *   **Score: 8/10** (The answer is mostly correct, but there might be some room for improvement in the explanation.)\n",
      "4.  **Doing no harm, and feeling no pain**:\n",
      "    *   The given answer is a kite. Kites are inanimate objects, so they don't feel any pain or discomfort.\n",
      "    *   **Score: 9/10** (The answer is mostly correct, but there might be some room for improvement in the explanation.)\n",
      "\n",
      "**Overall Score: 33/40**\n",
      "\n",
      "While the given answer is mostly correct, there's still room for improvement in the explanations. Let's think about how we can make the answer even better.\n",
      "\n",
      "**Improvement Suggestions**\n",
      "\n",
      "1.  **More specific language**: Try to use more specific language to explain each line of the riddle. For example, instead of saying \"only one color,\" you could say \"typically a single color, such as red, blue, or green.\"\n",
      "2.  **More detailed explanations**: Try to provide more detailed explanations for each line of the riddle. For example, instead of saying \"stuck at the bottom,\" you could say \"attached to a string or a stick at the bottom.\"\n",
      "3.  **More concrete examples**: Try to provide more concrete examples to illustrate each point. For example, instead of saying \"present in sun,\" you could say \"often flown in sunny weather, such as on a clear day with no clouds.\"\n",
      "\n",
      "By following these suggestions, we can make the answer even better and provide a more accurate and complete explanation.\n",
      "Q_value: -59.16666666666667\n",
      "evaluate_samples:\n",
      "[-80, -80, 45]\n",
      "\n",
      "node_4:\n",
      "parent: 2, children: []\n",
      "solution:\n",
      "The first clue states \"Only one color, but not one size.\" This means that the object has only one color, but it can come in different sizes. This is a common characteristic of kites, which are typically made of a single color, but can come in various sizes.\n",
      "\n",
      "The second clue says \"Stuck at the bottom, yet easily flies.\" This is a bit confusing, but it suggests that the object is attached to something at the bottom, but it can still move or fly easily. This description fits a kite, which is attached to a string at the bottom, but can still fly or move easily in the wind.\n",
      "\n",
      "The third clue states \"Present in sun, but not in rain.\" This means that the object is visible or present in sunny weather, but it disappears or is not visible in rainy weather. This is also true for kites, which are often visible in sunny weather, but may not be visible in rainy weather.\n",
      "\n",
      "The fourth clue says \"Doing no harm, and feeling no pain.\" This means that the object is harmless and doesn't feel any pain or discomfort. This is also true for kites, which are inanimate objects and don't feel any pain or discomfort.\n",
      "\n",
      "Given these clues, the most likely object that fits the description is a kite.\n",
      "\n",
      "[Final Answer] The answer is a kite.\n",
      "critique:\n",
      "I'll provide a critical analysis of the answer, pointing out flaws and imperfections.\n",
      "\n",
      "**Overall Score:** 4/10\n",
      "\n",
      "**Content Analysis:**\n",
      "\n",
      "1. **Answer clarity:** The answer is not clear or concise. The response tries to justify the answer, but it's not direct or straightforward.\n",
      "2. **Clue explanation:** The explanation for each clue is weak. The response tries to force the clues to fit the answer, rather than providing a logical and natural explanation.\n",
      "3. **Object description:** The object description is vague and does not provide any unique characteristics that distinguish it from other objects.\n",
      "\n",
      "**Specific Flaws:**\n",
      "\n",
      "1. **Clue 1:** The response states that the object has only one color, but it can come in different sizes. However, this is not a unique characteristic of kites, as many objects can come in different sizes with the same color.\n",
      "2. **Clue 2:** The response tries to justify the phrase \"Stuck at the bottom, yet easily flies\" by saying that the kite is attached to a string at the bottom. However, this is a stretch, as the phrase could be interpreted in many ways.\n",
      "3. **Clue 3:** The response states that the object is present in sun, but not in rain. While this is true for kites, it's not a unique characteristic that distinguishes kites from other objects.\n",
      "4. **Clue 4:** The response states that the object does no harm and feels no pain. While this is true for kites, it's not a characteristic that's unique to kites.\n",
      "\n",
      "**Suggestions for improvement:**\n",
      "\n",
      "1. **Simplify the answer:** Provide a clear and concise answer without justifying or explaining each clue.\n",
      "2. **Improve clue explanation:** Provide a logical and natural explanation for each clue that directly relates to the answer.\n",
      "3. **Provide unique characteristics:** Identify unique characteristics of the answer that distinguish it from other objects.\n",
      "4. **Use more precise language:** Avoid using vague language and use more precise language to describe the answer.\n",
      "\n",
      "**Grading Breakdown:**\n",
      "\n",
      "* Content analysis: 2/5\n",
      "* Clue explanation: 2/5\n",
      "* Object description: 1/5\n",
      "* Overall clarity: 1/5\n",
      "\n",
      "Note: The score is subjective and based on a critical analysis of the response.\n",
      "Q_value: -76.66666666666666\n",
      "evaluate_samples:\n",
      "[-80, -60, -80]\n",
      "\n",
      "node_5:\n",
      "parent: 2, children: []\n",
      "solution:\n",
      "[Reasoning Process]\n",
      "To solve this riddle, we need to analyze each line carefully and think about the possible connections between them. We should try to understand the meaning behind each clue and think about how they relate to each other.\n",
      "\n",
      "[Analysis]\n",
      "Let's break down the riddle and analyze each line:\n",
      "\n",
      "*   \"Only one color, but not one size\": This line suggests that the object has only one color, but it can come in different sizes. This means that the object is likely something that can be manufactured or produced in different sizes, but its color remains the same.\n",
      "*   \"Stuck at the bottom, yet easily flies\": This line is a bit contradictory, but it suggests that the object is stuck or attached to something at the bottom, but it can still fly or move easily. This means that the object is likely something that can be attached to a surface or a structure, but it can still move or fly.\n",
      "*   \"Present in sun, but not in rain\": This line suggests that the object is present or visible in sunny weather, but it disappears or is not visible in rainy weather. This means that the object is likely something that is sensitive to weather conditions or that is affected by the presence or absence of sunlight.\n",
      "*   \"Doing no harm, and feeling no pain\": This line suggests that the object is harmless and doesn't feel any pain or discomfort. This means that the object is likely something that is inanimate or doesn't have feelings or emotions.\n",
      "\n",
      "[Verification]\n",
      "Based on the analysis above, let's think about possible objects that fit the description. A possible object that comes to mind is a kite. Kites are typically made of a single color, but they can come in different sizes. They are often stuck or attached to a string at the bottom, but they can still fly or move easily in the wind. Kites are present in sunny weather, but they may not be visible in rainy weather. Finally, kites are inanimate objects that don't feel any pain or discomfort.\n",
      "\n",
      "[Final Answer] The answer is a kite.\n",
      "critique:\n",
      "The flaws in this reasoning are:\n",
      "\n",
      "1.  **Inconsistent Interpretation**: The answer provided (kite) doesn't perfectly match the clues. For instance, the kite can be damaged by the wind or collide with objects, which contradicts the claim that it \"feels no pain.\"\n",
      "2.  **Lack of Logical Connection**: The reasoning jumps from analyzing the clues to directly proposing a solution without clearly explaining how each clue relates to the answer. A more rigorous approach would involve systematically matching each clue to its corresponding aspect of the answer.\n",
      "3.  **Overemphasis on Single Clue**: The analysis focuses heavily on one clue (\"Only one color, but not one size\") without adequately addressing the others. A comprehensive solution would need to reconcile all clues with the proposed answer.\n",
      "4.  **Uncritical Acceptance of Assumptions**: The reasoning assumes that the object must be manufactured or produced in different sizes, which may not necessarily be true. A more nuanced approach would consider alternative explanations for the clues.\n",
      "5.  **Lack of Alternatives Consideration**: The solution doesn't explore other possible answers that might fit the description. A more thorough analysis would involve considering multiple options and evaluating their fit to the clues.\n",
      "6.  **Insufficient Rationale for Conclusion**: The final answer (\"kite\") is proposed without a clear explanation of how it addresses each of the clues. A more convincing solution would require a detailed breakdown of how each clue supports the answer.\n",
      "7.  **Overlooked Clue Implications**: The analysis doesn't fully consider the implications of some clues. For instance, the clue \"Present in sun, but not in rain\" could suggest an object that is sensitive to weather conditions or that is affected by the presence or absence of sunlight, but this aspect is not fully explored.\n",
      "8.  **Inadequate Handling of Contradictions**: The reasoning doesn't adequately address the apparent contradictions within the clues, such as the statement about the object being \"stuck at the bottom\" and \"easily flies.\" A more robust solution would need to reconcile these seeming contradictions.\n",
      "\n",
      "To improve the answer, one should:\n",
      "\n",
      "1.  **Carefully Reinterpret the Clues**: Re-examine the clues and try to find a more consistent and logical interpretation.\n",
      "2.  **Consider a Broader Range of Options**: Explore alternative answers that might fit the description and evaluate their fit to the clues.\n",
      "3.  **Develop a More Comprehensive Argument**: Systematically match each clue to its corresponding aspect of the answer and provide a detailed explanation of how each clue supports the answer.\n",
      "4.  **Address Contradictions and Assumptions**: Directly address the apparent contradictions and assumptions within the clues and provide a clear explanation of how they are resolved or reconciled.\n",
      "5.  **Provide a Thorough Rationale**: Offer a detailed breakdown of how each clue supports the answer and why the proposed solution is the most fitting.\n",
      "Q_value: -55.833333333333336\n",
      "evaluate_samples:\n",
      "[-80, -85, 85]\n",
      "\n",
      "\n",
      "Comparison of MCTS, Direct, and Actual Answers:\n",
      "\n",
      "Question: Only one color, but not one size, Stuck at the bottom, yet easily flies. Present in sun, but not in rain, Doing no harm, and feeling no pain. What is it?\n",
      "Actual_answer: Your shadow\n",
      "MCTS_answer: [Reasoning Process]\n",
      "To solve this riddle, we need to analyze each line carefully and think about the possible connections between them. We should try to understand the meaning behind each clue and think about how they relate to each other.\n",
      "\n",
      "[Analysis]\n",
      "Let's break down the riddle and analyze each line:\n",
      "\n",
      "*   \"Only one color, but not one size\": This line suggests that the object has only one color, but it can come in different sizes. This means that the object is likely something that can be manufactured or produced in different sizes, but its color remains the same.\n",
      "*   \"Stuck at the bottom, yet easily flies\": This line is a bit contradictory, but it suggests that the object is stuck or attached to something at the bottom, but it can still fly or move easily. This means that the object is likely something that can be attached to a surface or a structure, but it can still move or fly.\n",
      "*   \"Present in sun, but not in rain\": This line suggests that the object is present or visible in sunny weather, but it disappears or is not visible in rainy weather. This means that the object is likely something that is sensitive to weather conditions or that is affected by the presence or absence of sunlight.\n",
      "*   \"Doing no harm, and feeling no pain\": This line suggests that the object is harmless and doesn't feel any pain or discomfort. This means that the object is likely something that is inanimate or doesn't have feelings or emotions.\n",
      "\n",
      "[Verification]\n",
      "Based on the analysis above, let's think about possible objects that fit the description. A possible object that comes to mind is a kite. Kites are typically made of a single color, but they can come in different sizes. They are often stuck or attached to a string at the bottom, but they can still fly or move easily in the wind. Kites are present in sunny weather, but they may not be visible in rainy weather. Finally, kites are inanimate objects that don't feel any pain or discomfort.\n",
      "\n",
      "[Final Answer] The answer is a kite.\n",
      "direct_answer: A rainbow.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 237\u001b[0m\n\u001b[0;32m    234\u001b[0m actual_answer \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# Generate MCTS answer\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m mcts_answer \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Generate direct answer\u001b[39;00m\n\u001b[0;32m    240\u001b[0m direct_answer \u001b[38;5;241m=\u001b[39m generate_direct_answer(llm, query)\n",
      "Cell \u001b[1;32mIn[1], line 188\u001b[0m, in \u001b[0;36mMCTS_REASONING_LLM.run\u001b[1;34m(self, query, iterations)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcts_init(query)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m best_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m n: n\u001b[38;5;241m.\u001b[39mQ_value)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_node\u001b[38;5;241m.\u001b[39msolution\n",
      "Cell \u001b[1;32mIn[1], line 176\u001b[0m, in \u001b[0;36mMCTS_REASONING_LLM.iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m current_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[previous_node]\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(current_node)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[current_node]\u001b[38;5;241m.\u001b[39mreward_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[current_node]\u001b[38;5;241m.\u001b[39mQ_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_q_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[current_node]\u001b[38;5;241m.\u001b[39mreward_samples)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[current_node]\u001b[38;5;241m.\u001b[39mvisit_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 119\u001b[0m, in \u001b[0;36mMCTS_REASONING_LLM.self_evaluate\u001b[1;34m(self, query, solution, num_samples)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m    108\u001b[0m             prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|start_header_id|>user<|end_header_id|>Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolution\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124mExample: [Analysis] The solution has calculation errors and lacks proper reasoning. [Score] -45<|eot_id|>\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 119\u001b[0m             response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m             text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    121\u001b[0m             score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_score_from_text(text)\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama.py:1904\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1842\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1866\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1867\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \n\u001b[0;32m   1870\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama.py:1837\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1837\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama.py:1322\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1320\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1321\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1322\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama.py:916\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m--> 916\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalize_nl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalize_nl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m     sample_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids[: sample_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[sample_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens, :]\n\u001b[0;32m    938\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama.py:817\u001b[0m, in \u001b[0;36mLlama.sample\u001b[1;34m(self, top_k, top_p, min_p, typical_p, temp, repeat_penalty, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_eta, mirostat_tau, penalize_nl, logits_processor, grammar, idx)\u001b[0m\n\u001b[0;32m    814\u001b[0m ridx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 817\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mridx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp_sampler:\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\_internals.py:824\u001b[0m, in \u001b[0;36mLlamaSampler.sample\u001b[1;34m(self, ctx, idx)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx: LlamaContext, idx: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_sampler_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class MCTS_NODE:\n",
    "    def __init__(self, parent, solution, critique, Q_value):\n",
    "        self.parent = parent\n",
    "        self.solution = solution\n",
    "        self.critique = critique\n",
    "        self.Q_value = Q_value\n",
    "        self.visit_count = 0\n",
    "        self.children = []\n",
    "        self.reward_samples = []\n",
    "        self.fully_expanded = False\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "    def __init__(self, model_path, max_child=5, c=1):\n",
    "        self.model = Llama(model_path, n_ctx=16384, n_gpu_layers=99, logits_all=False, verbose=False)\n",
    "        self.max_child = max_child\n",
    "        self.c = c\n",
    "        self.nodes = []\n",
    "        self.query = None\n",
    "        self.dummy_answers = [\n",
    "            \"I Don't Know\",\n",
    "            \"I can't understand this question.\",\n",
    "            \"I can't help with this question.\",\n",
    "            \"I don't know how to solve this question.\",\n",
    "            \"I don't know the answer to this question.\",\n",
    "            \"I don't know the answer to this question, sorry.\"\n",
    "        ]\n",
    "\n",
    "    def is_fully_expanded(self, idx):\n",
    "        if len(self.nodes[idx].children) >= self.max_child:\n",
    "            return True\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            if self.nodes[child_idx].Q_value > self.nodes[idx].Q_value:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_optimum_child(self, idx):\n",
    "        if not self.nodes[idx].children:\n",
    "            return -1\n",
    "        parent_visit_count = self.nodes[self.nodes[idx].parent].visit_count if self.nodes[idx].parent != -1 else 1\n",
    "        UCT = []\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            child_visit_count = self.nodes[child_idx].visit_count\n",
    "            uct_value = (self.nodes[child_idx].Q_value +\n",
    "                         self.c * math.sqrt(math.log(parent_visit_count + 1) / (child_visit_count + 1e-6)))\n",
    "            UCT.append(uct_value)\n",
    "        if not self.nodes[idx].fully_expanded and np.max(UCT) < self.c * math.sqrt(math.log(parent_visit_count + 1) / 1e-6):\n",
    "            return -1\n",
    "        return self.nodes[idx].children[np.argmax(UCT)]\n",
    "\n",
    "    def generate_critique(self, query, solution):\n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Since we have a weak Answer, could you provide me with a reflection or feedback to correct this answer better? Analyze this Answer Strictly and Critically, point out every flaw for every possible imperfect to minus every possible score!\n",
    "\n",
    "Question: {query}\n",
    "Answer: {solution}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=1024, temperature=0.8)\n",
    "            return response[\"choices\"][0][\"text\"].strip()\n",
    "        except:\n",
    "            return \"The answer needs improvement.\"\n",
    "\n",
    "    def generate_refined_solution(self, query, original_solution, critique):\n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Please refine your answer according to the Reflection or Feedback. The response should begin with [reasoning process]...[Verification]... and end with \"[Final Answer] The answer is [answer formula]\"\n",
    "\n",
    "Question: {query}\n",
    "Original Answer: {original_solution}\n",
    "Feedback: {critique}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=2048, temperature=0.8)\n",
    "            return response[\"choices\"][0][\"text\"].strip()\n",
    "        except:\n",
    "            return original_solution\n",
    "\n",
    "    def extract_score_from_text(self, text):\n",
    "        score_patterns = [\n",
    "            r'\\[Score\\]\\s*[-]?\\d+',\n",
    "            r'\\[Score\\]:\\s*[-]?\\d+',\n",
    "            r'Score:\\s*[-]?\\d+',\n",
    "            r'Score\\s+[-]?\\d+',\n",
    "            r'score\\s*[:=]\\s*[-]?\\d+',\n",
    "        ]\n",
    "        for pattern in score_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                number_match = re.search(r'[-]?\\d+', match.group())\n",
    "                if number_match:\n",
    "                    return int(number_match.group())\n",
    "        all_numbers = re.findall(r'[-]?\\d+', text)\n",
    "        if all_numbers:\n",
    "            valid_scores = [int(num) for num in all_numbers if -100 <= int(num) <= 100]\n",
    "            if valid_scores:\n",
    "                return valid_scores[-1]\n",
    "        return 0\n",
    "\n",
    "    def self_evaluate(self, query, solution, num_samples=3):\n",
    "        scores = []\n",
    "        for _ in range(num_samples):\n",
    "            prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {query}\n",
    "Answer: {solution}\n",
    "\n",
    "Analyze this Answer Strictly and Critically, and point out every flaw for every possible imperfect to minus every possible score! You need to be very harsh and mean in calculating grades, and never give full marks to ensure that the marks are authoritative.\n",
    "\n",
    "Output a score between [-100,+100].\n",
    "\n",
    "Format: [Analysis] your analysis here [Score] your_number_here\n",
    "\n",
    "Example: [Analysis] The solution has calculation errors and lacks proper reasoning. [Score] -45<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "            response = self.model(prompt=prompt, max_tokens=512, temperature=0.8)\n",
    "            text = response[\"choices\"][0][\"text\"]\n",
    "            score = self.extract_score_from_text(text)\n",
    "            if score > 95:\n",
    "                score = max(95, score - 10)\n",
    "            score = max(-100, min(100, score))\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "    def calculate_q_value(self, reward_samples):\n",
    "        if not reward_samples:\n",
    "            return 0\n",
    "        min_reward = min(reward_samples)\n",
    "        mean_reward = sum(reward_samples) / len(reward_samples)\n",
    "        return 0.5 * (min_reward + mean_reward)\n",
    "\n",
    "    def update_q_value_with_children(self, node_index):\n",
    "        node = self.nodes[node_index]\n",
    "        base_q = self.calculate_q_value(node.reward_samples)\n",
    "        max_child_q = float('-inf')\n",
    "        has_children = False\n",
    "        for child_idx in node.children:\n",
    "            child_q = self.nodes[child_idx].Q_value\n",
    "            max_child_q = max(max_child_q, child_q)\n",
    "            has_children = True\n",
    "        if has_children:\n",
    "            node.Q_value = 0.5 * (base_q + max_child_q)\n",
    "        else:\n",
    "            node.Q_value = base_q\n",
    "\n",
    "    def mcts_init(self, query):\n",
    "        self.query = query\n",
    "        self.nodes = []\n",
    "        dummy_solution = random.choice(self.dummy_answers)\n",
    "        critique = self.generate_critique(self.query, dummy_solution)\n",
    "        root_node = MCTS_NODE(-1, dummy_solution, critique, 0)\n",
    "        root_node.reward_samples = self.self_evaluate(query, dummy_solution)\n",
    "        root_node.Q_value = self.calculate_q_value(root_node.reward_samples)\n",
    "        root_node.visit_count = 1\n",
    "        self.nodes.append(root_node)\n",
    "\n",
    "    def iterator(self):\n",
    "        current_node = 0\n",
    "        previous_node = -1\n",
    "        while current_node != -1:\n",
    "            previous_node = current_node\n",
    "            current_node = self.get_optimum_child(current_node)\n",
    "        solution = self.generate_refined_solution(\n",
    "            self.query,\n",
    "            self.nodes[previous_node].solution,\n",
    "            self.nodes[previous_node].critique\n",
    "        )\n",
    "        critique = self.generate_critique(self.query, solution)\n",
    "        new_node = MCTS_NODE(previous_node, solution, critique, 0)\n",
    "        self.nodes.append(new_node)\n",
    "        current_node = len(self.nodes) - 1\n",
    "        self.nodes[previous_node].children.append(current_node)\n",
    "        self.nodes[current_node].reward_samples = self.self_evaluate(self.query, solution)\n",
    "        self.nodes[current_node].Q_value = self.calculate_q_value(self.nodes[current_node].reward_samples)\n",
    "        self.nodes[current_node].visit_count = 1\n",
    "        while previous_node != -1:\n",
    "            self.nodes[previous_node].visit_count += 1\n",
    "            self.update_q_value_with_children(previous_node)\n",
    "            self.nodes[previous_node].fully_expanded = self.is_fully_expanded(previous_node)\n",
    "            previous_node = self.nodes[previous_node].parent\n",
    "\n",
    "    def run(self, query, iterations=5):\n",
    "        self.mcts_init(query)\n",
    "        for _ in range(iterations):\n",
    "            self.iterator()\n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        return best_node.solution\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        if not self.nodes:\n",
    "            return None\n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        return best_node.solution\n",
    "    \n",
    "    def print_status(self):\n",
    "        print(\"printing MCTS status\")\n",
    "        idx = 0\n",
    "        for nodes in self.nodes:\n",
    "            print(f\"node_{idx}:\\n\"\n",
    "                  f\"parent: {nodes.parent}, children: {nodes.children}\\n\"\n",
    "                  f\"solution:\\n\"\n",
    "                  f\"{nodes.solution}\\n\"\n",
    "                  \"critique:\\n\"\n",
    "                  f\"{nodes.critique}\\n\"\n",
    "                  f\"Q_value: {nodes.Q_value}\\n\"\n",
    "                  f\"evaluate_samples:\\n\"\n",
    "                  f\"{nodes.reward_samples}\\n\")\n",
    "            idx += 1\n",
    "\n",
    "# Function to generate direct answer using Llama\n",
    "def generate_direct_answer(model, query):\n",
    "    prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {query}\n",
    "Please provide the answer directly without any reasoning or explanation.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    try:\n",
    "        response = model(prompt=prompt, max_tokens=512, temperature=0.8)\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except:\n",
    "        return \"Unable to generate answer.\"\n",
    "\n",
    "# Load riddle data\n",
    "df = pd.read_excel(\"Riddle.xlsx\")\n",
    "\n",
    "# Initialize Llama model (replace with actual model path)\n",
    "mcts = MCTS_REASONING_LLM(model_path=\"llama-3.1-8b-instruct-q4_k_m.gguf\", max_child = 2)\n",
    "llm = mcts.model\n",
    "# Test on a subset of riddles (e.g., first 5 for demonstration)\n",
    "results = []\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    query = row[\"Question\"]\n",
    "    actual_answer = row[\"Answer\"]\n",
    "    \n",
    "    # Generate MCTS answer\n",
    "    mcts_answer = mcts.run(query, iterations=10)\n",
    "    \n",
    "    # Generate direct answer\n",
    "    direct_answer = generate_direct_answer(llm, query)\n",
    "\n",
    "    # mcts.print_status()\n",
    "\n",
    "    print(\"\\nComparison of MCTS, Direct, and Actual Answers:\\n\")\n",
    "    print(f\"Question: {query}\\n\"\n",
    "          f\"Actual_answer: {actual_answer}\\n\"\n",
    "          f\"MCTS_answer: {mcts_answer}\\n\"\n",
    "          f\"direct_answer: {direct_answer}\\n\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"Question\": query,\n",
    "        \"Actual Answer\": actual_answer,\n",
    "        \"MCTS Answer\": mcts_answer,\n",
    "        \"Direct Answer\": direct_answer\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"riddle_comparison.csv\", index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nComparison of MCTS, Direct, and Actual Answers:\")\n",
    "print(results_df[[\"ID\", \"Question\", \"Actual Answer\", \"MCTS Answer\", \"Direct Answer\"]].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
