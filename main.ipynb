{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1422dbe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.3.14\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_grammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaGrammar\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     BaseLlamaCache,\n\u001b[0;32m     37\u001b[0m     LlamaCache,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     LlamaDiskCache,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     LlamaRAMCache,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLlamaTokenizer, LlamaTokenizer\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama_cache.py:10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     Optional,\n\u001b[0;32m      5\u001b[0m     Sequence,\n\u001b[0;32m      6\u001b[0m     Tuple,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdiskcache\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diskcache\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mDiskCache API Reference\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m=======================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThe :doc:`tutorial` provides a helpful walkthrough of most methods.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     DEFAULT_SETTINGS,\n\u001b[0;32m     10\u001b[0m     ENOVAL,\n\u001b[0;32m     11\u001b[0m     EVICTION_POLICY,\n\u001b[0;32m     12\u001b[0m     UNKNOWN,\n\u001b[0;32m     13\u001b[0m     Cache,\n\u001b[0;32m     14\u001b[0m     Disk,\n\u001b[0;32m     15\u001b[0m     EmptyDirWarning,\n\u001b[0;32m     16\u001b[0m     JSONDisk,\n\u001b[0;32m     17\u001b[0m     Timeout,\n\u001b[0;32m     18\u001b[0m     UnknownFileWarning,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfanout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FanoutCache\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpersistent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deque, Index\n",
      "File \u001b[1;32mc:\\Users\\PhilipZhu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diskcache\\core.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mop\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickletools\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstruct\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stream_chat_bot:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.llm = Llama(model_path=self.model_path, n_ctx=16384, n_gpu_layers=99)\n",
    "        self.clean()\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "\n",
    "    def answer(self, query):\n",
    "        self.push_back(\"user\", query)\n",
    "        output = self.llm(self.stream + \"<|start_header_id|>assistant<|end_header_id|>\", max_tokens=2048)\n",
    "        self.push_back(\"assistant\", f\"{output['choices'][0]['text']}\")\n",
    "        return output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "\n",
    "    class MCTS_NODE:\n",
    "        def __init__(self,parent,actions_and_probs,k):\n",
    "            self.parent = parent\n",
    "            self.last_visit = -1\n",
    "            self.end = False\n",
    "            (self.actions, self.probs) = actions_and_probs\n",
    "            self.actions = np.array(self.actions)\n",
    "            self.probs = np.array(self.probs)\n",
    "            self.child = np.full((k,),-1)\n",
    "            self.mean_reward = np.full((k,),0)\n",
    "            self.visit_count = 0\n",
    "            self.child_visit_count = np.full((k,),0)\n",
    "\n",
    "        def get_optimum_child(self, policy_weight, explore_weight):\n",
    "            score = self.mean_reward + self.probs*policy_weight + \\\n",
    "                (np.sqrt(np.log(self.visit_count + 1e-6))/(self.child_visit_count + 1e-6))*explore_weight\n",
    "            return self.child[np.argmax(score)]\n",
    "\n",
    "    def __init__(self, model_path, k, policy_weight = 1, explore_weight = 1):\n",
    "        self.model = Llama(model_path, n_ctx = 16384, n_gpu_layers = 99,logits_all = True,verbose = False)\n",
    "        self.clean()\n",
    "        self.nodes = []\n",
    "        self.policy_weight = policy_weight\n",
    "        self.explore_weight = explore_weight   \n",
    "        self.k = k\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "    \n",
    "    def generate_action_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 256,\n",
    "                                      logprobs = 0, temperature = 0, stop='\\n')[\"choices\"][0][\"text\"] + \"\\n\")\n",
    "        return (actions, probs)\n",
    "    \n",
    "    def generate_answer_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 512,\n",
    "                                      logprobs = 0, temperature = 0)[\"choices\"][0][\"text\"])\n",
    "        return (actions, probs)\n",
    "    \n",
    "    # given a query and an answer, evaluate the answer\n",
    "    # this function is a placeholder and should be implemented based on the specific evaluation criteria\n",
    "    def answer_evaluate(self, query_answer):\n",
    "        pass\n",
    "    \n",
    "    def MCTS_initialize(self):\n",
    "        self.nodes = []\n",
    "        self.deleted = []\n",
    "        self.nodes.append(MCTS_REASONING_LLM.MCTS_NODE(-1, self.generate_action_list(self.stream), self.k))\n",
    "        self.set_root(0)\n",
    "\n",
    "    def new_node(self, parent, actions_and_probs):\n",
    "        new_node = MCTS_REASONING_LLM.MCTS_NODE(parent, actions_and_probs, self.k)\n",
    "        if(len(self.deleted)):\n",
    "            self.nodes[self.deleted[0]] = new_node\n",
    "            idx = self.deleted[0]\n",
    "            self.deleted = self.deleted[1:]\n",
    "            return idx \n",
    "        else:\n",
    "            self.nodes.append(new_node)\n",
    "            return len(self.nodes) - 1\n",
    "    \n",
    "    def delete_node(self, idx):\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def delete_tree(self, idx):\n",
    "        for child in self.nodes[idx].child:\n",
    "            if(child != -1):\n",
    "                self.delete_tree(child)\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def set_root(self,idx):\n",
    "        self.root = idx\n",
    "        self.nodes[idx].parent = -1\n",
    "    \n",
    "    def select_and_expand(self):\n",
    "        previous_node = -1\n",
    "        current_node = self.root\n",
    "        last_visit = -1\n",
    "        current_prompt = self.stream\n",
    "        while current_node != -1 and self.nodes[current_node].end == False:\n",
    "            last_visit = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = last_visit\n",
    "            current_prompt += self.nodes[current_node].actions[last_visit]\n",
    "            previous_node = current_node\n",
    "            current_node = self.nodes[current_node].child[last_visit]\n",
    "\n",
    "        if current_node != -1:\n",
    "            return current_node, current_prompt\n",
    "        else:\n",
    "            if self.nodes[previous_node].actions[last_visit] == \"[answer]\\n\":\n",
    "                actions, probs = self.generate_answer_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "                self.nodes[current_node].end = True\n",
    "                for i in range(len(self.nodes[current_node].child)):\n",
    "                    self.nodes[current_node].mean_reward[i] = self.answer_evaluate(self.stream + self.nodes[current_node].actions[i] + '<|eot_id|>')\n",
    "            else:\n",
    "                actions, probs = self.generate_action_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "            return current_node, current_prompt\n",
    "        \n",
    "    def simulation(self, current_node, current_prompt):\n",
    "        if self.nodes[current_node].end:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reward = self.nodes[current_node].mean_reward[optimum_child]\n",
    "            return reward\n",
    "        else:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reasoning = self.nodes[current_node].actions[optimum_child]\n",
    "            while(reasoning != \"[answer]\\n\"):\n",
    "                current_prompt += reasoning\n",
    "                reasoning = self.model(prompt = current_prompt, max_tokens = 256, temperature = 0, stop = '\\n', logprobs = 0)[\"choices\"][0][\"text\"] + \"\\n\"\n",
    "            current_prompt += reasoning\n",
    "            respond = self.model(prompt = current_prompt, max_tokens = 512, temperature = 0,  logprobs = 0)[\"choices\"][0][\"text\"] + \"<|eot_id|>\"\n",
    "            return self.answer_evaluate(self.stream + respond + '<|eot_id|>')\n",
    "            \n",
    "    def backpropagation(self, current_node, reward):\n",
    "        while current_node != -1:\n",
    "            self.nodes[current_node].visit_count += 1\n",
    "            self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit] += 1\n",
    "            self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit] += \\\n",
    "            (reward - self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit]) / self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit]\n",
    "            current_node = self.nodes[current_node].parent\n",
    "\n",
    "    def query(self,query,iterations=100):\n",
    "        self.push_back(\"user\",query)\n",
    "        self.stream += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.MCTS_initialize()\n",
    "        while(not self.nodes[self.root].end):\n",
    "            while self.nodes[self.root].visit_count < iterations :\n",
    "                current_node, current_prompt = self.select_and_expand()\n",
    "                reward = self.simulation(current_node, current_prompt)\n",
    "                self.backpropagation(current_node, reward)\n",
    "            optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "            self.stream += self.nodes[self.root].actions[optimum_child]\n",
    "            for i in range(self.k):\n",
    "                if i != optimum_child and i != -1:\n",
    "                    self.delete_tree(self.nodes[self.root].child[i])\n",
    "            new_root = self.nodes[self.root].child[optimum_child]\n",
    "            self.delete_node(self.root)\n",
    "            self.set_root(new_root)\n",
    "        optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "        respond = self.nodes[self.root].actions[optimum_child]\n",
    "        self.stream += respond + '<|eot_id|>'\n",
    "        return respond\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "\n",
    "    class MCTS_NODE:\n",
    "        def __init__(self,parent,actions_and_probs,k):\n",
    "            self.parent = parent\n",
    "            self.last_visit = -1\n",
    "            self.end = False\n",
    "            (self.actions, self.probs) = actions_and_probs\n",
    "            self.actions = np.array(self.actions)\n",
    "            self.probs = np.array(self.probs)\n",
    "            self.child = np.full((k,),-1)\n",
    "            self.mean_reward = np.full((k,),0)\n",
    "            self.visit_count = 0\n",
    "            self.child_visit_count = np.full((k,),0)\n",
    "\n",
    "        def get_optimum_child(self, policy_weight, explore_weight):\n",
    "            score = self.mean_reward + self.probs*policy_weight + \\\n",
    "                (np.sqrt(np.log(self.visit_count + 1e-6))/(self.child_visit_count + 1e-6))*explore_weight\n",
    "            return self.child[np.argmax(score)]\n",
    "\n",
    "    def __init__(self, model_path, k, policy_weight = 1, explore_weight = 1):\n",
    "        self.model = Llama(model_path, n_ctx = 16384, n_gpu_layers = 99,logits_all = True,verbose = False)\n",
    "        self.clean()\n",
    "        self.nodes = []\n",
    "        self.policy_weight = policy_weight\n",
    "        self.explore_weight = explore_weight   \n",
    "        self.k = k\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "    \n",
    "    def generate_action_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 256,\n",
    "                                      logprobs = 0, temperature = 0, stop='\\n')[\"choices\"][0][\"text\"] + \"\\n\")\n",
    "        return (actions, probs)\n",
    "    \n",
    "    def generate_answer_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 512,\n",
    "                                      logprobs = 0, temperature = 0)[\"choices\"][0][\"text\"])\n",
    "        return (actions, probs)\n",
    "    \n",
    "    # given a query and an answer, evaluate the answer\n",
    "    # this function is a placeholder and should be implemented based on the specific evaluation criteria\n",
    "    def answer_evaluate(self, query_answer):\n",
    "        return 1\n",
    "    \n",
    "    def MCTS_initialize(self):\n",
    "        self.nodes = []\n",
    "        self.deleted = []\n",
    "        self.nodes.append(MCTS_REASONING_LLM.MCTS_NODE(-1, self.generate_action_list(self.stream), self.k))\n",
    "        self.set_root(0)\n",
    "\n",
    "    def new_node(self, parent, actions_and_probs):\n",
    "        new_node = MCTS_REASONING_LLM.MCTS_NODE(parent, actions_and_probs, self.k)\n",
    "        if(len(self.deleted)):\n",
    "            self.nodes[self.deleted[0]] = new_node\n",
    "            idx = self.deleted[0]\n",
    "            self.deleted = self.deleted[1:]\n",
    "            return idx \n",
    "        else:\n",
    "            self.nodes.append(new_node)\n",
    "            return len(self.nodes) - 1\n",
    "    \n",
    "    def delete_node(self, idx):\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def delete_tree(self, idx):\n",
    "        for child in self.nodes[idx].child:\n",
    "            if(child != -1):\n",
    "                self.delete_tree(child)\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def set_root(self,idx):\n",
    "        self.root = idx\n",
    "        self.nodes[idx].parent = -1\n",
    "    \n",
    "    def select_and_expand(self):\n",
    "        print(\"select_and_expand\")\n",
    "        previous_node = -1\n",
    "        current_node = self.root\n",
    "        last_visit = -1\n",
    "        current_prompt = self.stream\n",
    "        while current_node != -1 and self.nodes[current_node].end == False:\n",
    "            print(\"Current Node:\", current_node, \"End:\", self.nodes[current_node].end)\n",
    "            last_visit = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = last_visit\n",
    "            current_prompt += self.nodes[current_node].actions[last_visit]\n",
    "            previous_node = current_node\n",
    "            current_node = self.nodes[current_node].child[last_visit]\n",
    "\n",
    "        print(\"Final Node:\", current_node, \"End:\", self.nodes[current_node].end)\n",
    "\n",
    "        if current_node != -1:\n",
    "            print(\"Returning current node:\", current_node, \"with prompt:\", current_prompt)\n",
    "            return current_node, current_prompt\n",
    "        else:\n",
    "            if self.nodes[previous_node].actions[last_visit] == \"[answer]\\n\":\n",
    "                actions, probs = self.generate_answer_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "                self.nodes[current_node].end = True\n",
    "                for i in range(len(self.nodes[current_node].child)):\n",
    "                    self.nodes[current_node].mean_reward[i] = self.answer_evaluate(self.stream + self.nodes[current_node].actions[i] + '<|eot_id|>')\n",
    "            else:\n",
    "                actions, probs = self.generate_action_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "            print(\"Returning new node:\", current_node, \"with prompt:\", current_prompt)\n",
    "            return current_node, current_prompt\n",
    "        \n",
    "    def simulation(self, current_node, current_prompt):\n",
    "        print(\"simulation for node:\", current_node)\n",
    "        if self.nodes[current_node].end:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reward = self.nodes[current_node].mean_reward[optimum_child]\n",
    "            print(\"Returning reward:\", reward)\n",
    "            return reward\n",
    "        else:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reasoning = self.nodes[current_node].actions[optimum_child]\n",
    "            while(reasoning != \"[answer]\\n\"):\n",
    "                print(\"Current Reasoning:\", reasoning)\n",
    "                current_prompt += reasoning\n",
    "                reasoning = self.model(prompt = current_prompt, max_tokens = 256, temperature = 0, stop = '\\n', logprobs = 0)[\"choices\"][0][\"text\"] + \"\\n\"\n",
    "            current_prompt += reasoning\n",
    "            respond = self.model(prompt = current_prompt, max_tokens = 512, temperature = 0,  logprobs = 0)[\"choices\"][0][\"text\"] + \"<|eot_id|>\"\n",
    "            print(\"Returning response:\", respond)\n",
    "            return self.answer_evaluate(self.stream + respond + '<|eot_id|>')\n",
    "            \n",
    "    def backpropagation(self, current_node, reward):\n",
    "        while current_node != -1:\n",
    "            print(\"Backpropagating for node:\", current_node, \"with reward:\", reward)\n",
    "            self.nodes[current_node].visit_count += 1\n",
    "            self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit] += 1\n",
    "            self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit] += \\\n",
    "            (reward - self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit]) / self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit]\n",
    "            current_node = self.nodes[current_node].parent\n",
    "\n",
    "    def query(self,query,iterations=100):\n",
    "        print(\"Querying:\", query)\n",
    "        self.push_back(\"user\",query)\n",
    "        self.stream += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.MCTS_initialize()\n",
    "        while(not self.nodes[self.root].end):\n",
    "            print(\"Root Node:\", self.root, \"End:\", self.nodes[self.root].end, \"Visit Count:\", self.nodes[self.root].visit_count)\n",
    "            while self.nodes[self.root].visit_count < iterations :\n",
    "                print(\"Running MCTS iterations, current visit count:\", self.nodes[self.root].visit_count)\n",
    "                current_node, current_prompt = self.select_and_expand()\n",
    "                reward = self.simulation(current_node, current_prompt)\n",
    "                self.backpropagation(current_node, reward)\n",
    "            optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "            self.stream += self.nodes[self.root].actions[optimum_child]\n",
    "            for i in range(self.k):\n",
    "                if i != optimum_child and i != -1:\n",
    "                    self.delete_tree(self.nodes[self.root].child[i])\n",
    "            new_root = self.nodes[self.root].child[optimum_child]\n",
    "            self.delete_node(self.root)\n",
    "            self.set_root(new_root)\n",
    "        optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "        respond = self.nodes[self.root].actions[optimum_child]\n",
    "        self.stream += respond + '<|eot_id|>'\n",
    "        return respond\n",
    "            \n",
    "\n",
    "model_path = \"llama-3.2-1b-instruct-q4_k_m.gguf\"\n",
    "mcts_llm = MCTS_REASONING_LLM(model_path, k=3, policy_weight=1, explore_weight=1)\n",
    "query = \"1 + 1 - 2 = ?\"\n",
    "response = mcts_llm.query(query, iterations=3)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "\n",
    "    class MCTS_NODE:\n",
    "        def __init__(self,parent,actions_and_probs,k):\n",
    "            self.parent = parent\n",
    "            self.last_visit = -1\n",
    "            self.end = False\n",
    "            (self.actions, self.probs) = actions_and_probs\n",
    "            self.actions = np.array(self.actions)\n",
    "            self.probs = np.array(self.probs)\n",
    "            self.child = np.full((k,),-1)\n",
    "            self.mean_reward = np.full((k,),0)\n",
    "            self.visit_count = 0\n",
    "            self.child_visit_count = np.full((k,),0)\n",
    "\n",
    "        def get_optimum_child(self, policy_weight, explore_weight):\n",
    "            score = self.mean_reward + self.probs*policy_weight + \\\n",
    "                (np.sqrt(np.log(self.visit_count + 1e-6))/(self.child_visit_count + 1e-6))*explore_weight\n",
    "            return self.child[np.argmax(score)]\n",
    "\n",
    "    def __init__(self, model_path, k, policy_weight = 1, explore_weight = 1):\n",
    "        self.model = Llama(model_path, n_ctx = 16384, n_gpu_layers = 99,logits_all = True,verbose = False)\n",
    "        self.clean()\n",
    "        self.nodes = []\n",
    "        self.policy_weight = policy_weight\n",
    "        self.explore_weight = explore_weight   \n",
    "        self.k = k\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "    \n",
    "    def generate_action_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 256,\n",
    "                                      logprobs = 0, temperature = 0, stop='\\n')[\"choices\"][0][\"text\"] + \"\\n\")\n",
    "        return (actions, probs)\n",
    "    \n",
    "    def generate_answer_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 512,\n",
    "                                      logprobs = 0, temperature = 0)[\"choices\"][0][\"text\"])\n",
    "        return (actions, probs)\n",
    "    \n",
    "    # given a query and an answer, evaluate the answer\n",
    "    # this function is a placeholder and should be implemented based on the specific evaluation criteria\n",
    "    def answer_evaluate(self, query_answer):\n",
    "        pass\n",
    "    \n",
    "    def MCTS_initialize(self):\n",
    "        self.nodes = []\n",
    "        self.deleted = []\n",
    "        self.nodes.append(MCTS_REASONING_LLM.MCTS_NODE(-1, self.generate_action_list(self.stream), self.k))\n",
    "        self.set_root(0)\n",
    "\n",
    "    def new_node(self, parent, actions_and_probs):\n",
    "        new_node = MCTS_REASONING_LLM.MCTS_NODE(parent, actions_and_probs, self.k)\n",
    "        if(len(self.deleted)):\n",
    "            self.nodes[self.deleted[0]] = new_node\n",
    "            idx = self.deleted[0]\n",
    "            self.deleted = self.deleted[1:]\n",
    "            return idx \n",
    "        else:\n",
    "            self.nodes.append(new_node)\n",
    "            return len(self.nodes) - 1\n",
    "    \n",
    "    def delete_node(self, idx):\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def delete_tree(self, idx):\n",
    "        for child in self.nodes[idx].child:\n",
    "            if(child != -1):\n",
    "                self.delete_tree(child)\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def set_root(self,idx):\n",
    "        self.root = idx\n",
    "        self.nodes[idx].parent = -1\n",
    "    \n",
    "    def select_and_expand(self):\n",
    "        previous_node = -1\n",
    "        current_node = self.root\n",
    "        last_visit = -1\n",
    "        current_prompt = self.stream\n",
    "        while current_node != -1 and self.nodes[current_node].end == False:\n",
    "            last_visit = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = last_visit\n",
    "            current_prompt += self.nodes[current_node].actions[last_visit]\n",
    "            previous_node = current_node\n",
    "            current_node = self.nodes[current_node].child[last_visit]\n",
    "\n",
    "        if current_node != -1:\n",
    "            return current_node, current_prompt\n",
    "        else:\n",
    "            if self.nodes[previous_node].actions[last_visit] == \"[answer]\\n\":\n",
    "                actions, probs = self.generate_answer_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "                self.nodes[current_node].end = True\n",
    "                for i in range(len(self.nodes[current_node].child)):\n",
    "                    self.nodes[current_node].mean_reward[i] = self.answer_evaluate(self.stream + self.nodes[current_node].actions[i] + '<|eot_id|>')\n",
    "            else:\n",
    "                actions, probs = self.generate_action_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "            return current_node, current_prompt\n",
    "        \n",
    "    def simulation(self, current_node, current_prompt):\n",
    "        if self.nodes[current_node].end:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reward = self.nodes[current_node].mean_reward[optimum_child]\n",
    "            return reward\n",
    "        else:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reasoning = self.nodes[current_node].actions[optimum_child]\n",
    "            while(reasoning != \"[answer]\\n\"):\n",
    "                current_prompt += reasoning\n",
    "                reasoning = self.model(prompt = current_prompt, max_tokens = 256, temperature = 0, stop = '\\n', logprobs = 0)[\"choices\"][0][\"text\"] + \"\\n\"\n",
    "            current_prompt += reasoning\n",
    "            respond = self.model(prompt = current_prompt, max_tokens = 512, temperature = 0,  logprobs = 0)[\"choices\"][0][\"text\"] + \"<|eot_id|>\"\n",
    "            return self.answer_evaluate(self.stream + respond + '<|eot_id|>')\n",
    "            \n",
    "    def backpropagation(self, current_node, reward):\n",
    "        while current_node != -1:\n",
    "            self.nodes[current_node].visit_count += 1\n",
    "            self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit] += 1\n",
    "            self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit] += \\\n",
    "            (reward - self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit]) / self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit]\n",
    "            current_node = self.nodes[current_node].parent\n",
    "\n",
    "    def query(self,query,iterations=100):\n",
    "        self.push_back(\"user\",query)\n",
    "        self.stream += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.MCTS_initialize()\n",
    "        while(not self.nodes[self.root].end):\n",
    "            while self.nodes[self.root].visit_count < iterations :\n",
    "                current_node, current_prompt = self.select_and_expand()\n",
    "                reward = self.simulation(current_node, current_prompt)\n",
    "                self.backpropagation(current_node, reward)\n",
    "            optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "            self.stream += self.nodes[self.root].actions[optimum_child]\n",
    "            for i in range(self.k):\n",
    "                if i != optimum_child and i != -1:\n",
    "                    self.delete_tree(self.nodes[self.root].child[i])\n",
    "            new_root = self.nodes[self.root].child[optimum_child]\n",
    "            self.delete_node(self.root)\n",
    "            self.set_root(new_root)\n",
    "        optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "        respond = self.nodes[self.root].actions[optimum_child]\n",
    "        self.stream += respond + '<|eot_id|>'\n",
    "        return respond\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60143f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "model = Llama(\"llama-3.1-8b-instruct-q4_k_m.gguf\", n_ctx = 16384, n_gpu_layers = 99,logits_all = True,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n",
      "<|start_header_id|>user<|end_header_id|>1 + 1 = ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "1 + 1 = 2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"<|start_header_id|>user<|end_header_id|>1 + 1 = ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "response = model.create_completion(prompt = prompt,stream = False,max_tokens = 1024,echo = True,logprobs=True,stop=[\"\\n\",])\n",
    "print(response[\"choices\"][0][\"finish_reason\"])\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
