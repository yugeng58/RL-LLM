{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stream_chat_bot:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.llm = Llama(model_path=self.model_path, n_ctx=16384, n_gpu_layers=99,verbose=False)\n",
    "        self.clean()\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "\n",
    "    def answer(self, query):\n",
    "        self.push_back(\"user\", query)\n",
    "        output = self.llm(self.stream + \"<|start_header_id|>assistant<|end_header_id|>\", max_tokens=2048)\n",
    "        self.push_back(\"assistant\", f\"{output['choices'][0]['text']}\")\n",
    "        return output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d88f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = stream_chat_bot(\"llama-3.1-8b-instruct-q4_k_m.gguf\")\n",
    "while True:\n",
    "    print(bot.answer(input(\"\\n>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bf04a",
   "metadata": {},
   "source": [
    "# sentence as node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "\n",
    "    class MCTS_NODE:\n",
    "        def __init__(self,parent,actions_and_probs,k):\n",
    "            self.parent = parent\n",
    "            self.last_visit = -1\n",
    "            self.end = False\n",
    "            (self.actions, self.probs) = actions_and_probs\n",
    "            self.actions = np.array(self.actions)\n",
    "            self.probs = np.array(self.probs)\n",
    "            self.child = np.full((k,),-1)\n",
    "            self.mean_reward = np.full((k,),0)\n",
    "            self.visit_count = 0\n",
    "            self.child_visit_count = np.full((k,),0)\n",
    "\n",
    "        def get_optimum_child(self, policy_weight, explore_weight):\n",
    "            score = self.mean_reward + self.probs*policy_weight + \\\n",
    "                (np.sqrt(np.log(self.visit_count + 1e-6))/(self.child_visit_count + 1e-6))*explore_weight\n",
    "            return self.child[np.argmax(score)]\n",
    "\n",
    "    def __init__(self, model_path, k, policy_weight = 1, explore_weight = 1):\n",
    "        self.model = Llama(model_path, n_ctx = 16384, n_gpu_layers = 99,logits_all = True,verbose = False)\n",
    "        self.clean()\n",
    "        self.nodes = []\n",
    "        self.policy_weight = policy_weight\n",
    "        self.explore_weight = explore_weight   \n",
    "        self.k = k\n",
    "\n",
    "    def push_back(self, role, content):\n",
    "        self.stream += f\" <|start_header_id|>{role}<|end_header_id|>\"\\\n",
    "                        f\"{content}\"\\\n",
    "                        \"<|eot_id|>\"\n",
    "\n",
    "    def clean(self):\n",
    "        self.stream = \"\"\n",
    "        self.push_back(\"system\", \"You are a helpful assistant. Always think step-by-step before answering and format your response as follows:\\n\"\n",
    "                                 \"<step 1 content>\\n\"\n",
    "                                 \"<step 2 content>\\n\"\n",
    "                                 \"...\\n\"\n",
    "                                 \"[answer]\\n\"\n",
    "                                 \"<answer content>\\n\"\n",
    "                                 \"Ensure every response follows this format, with each reasoning step on a new line and the answer preceded by [answer] on a new line, followed by its content on the next line.\")\n",
    "    \n",
    "    def generate_action_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 256,\n",
    "                                      logprobs = 0, temperature = 0, stop='\\n')[\"choices\"][0][\"text\"] + \"\\n\")\n",
    "        return (actions, probs)\n",
    "    \n",
    "    def generate_answer_list(self,prompt):\n",
    "        output = self.model(prompt = prompt,max_tokens = 1,logprobs = self.k,temperature=0)\n",
    "        output = output[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "        first_tokens = list(output.keys())\n",
    "        probs = list(output.values())\n",
    "        actions = []\n",
    "        for token in first_tokens:\n",
    "            actions.append(token + self.model(prompt = prompt + token, max_tokens = 512,\n",
    "                                      logprobs = 0, temperature = 0)[\"choices\"][0][\"text\"])\n",
    "        return (actions, probs)\n",
    "    \n",
    "    # given a query and an answer, evaluate the answer\n",
    "    # this function is a placeholder and should be implemented based on the specific evaluation criteria\n",
    "    def answer_evaluate(self, query_answer):\n",
    "        pass\n",
    "    \n",
    "    def MCTS_initialize(self):\n",
    "        self.nodes = []\n",
    "        self.deleted = []\n",
    "        self.nodes.append(MCTS_REASONING_LLM.MCTS_NODE(-1, self.generate_action_list(self.stream), self.k))\n",
    "        self.set_root(0)\n",
    "\n",
    "    def new_node(self, parent, actions_and_probs):\n",
    "        new_node = MCTS_REASONING_LLM.MCTS_NODE(parent, actions_and_probs, self.k)\n",
    "        if(len(self.deleted)):\n",
    "            self.nodes[self.deleted[0]] = new_node\n",
    "            idx = self.deleted[0]\n",
    "            self.deleted = self.deleted[1:]\n",
    "            return idx \n",
    "        else:\n",
    "            self.nodes.append(new_node)\n",
    "            return len(self.nodes) - 1\n",
    "    \n",
    "    def delete_node(self, idx):\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def delete_tree(self, idx):\n",
    "        for child in self.nodes[idx].child:\n",
    "            if(child != -1):\n",
    "                self.delete_tree(child)\n",
    "        self.deleted.append(idx)\n",
    "\n",
    "    def set_root(self,idx):\n",
    "        self.root = idx\n",
    "        self.nodes[idx].parent = -1\n",
    "    \n",
    "    def select_and_expand(self):\n",
    "        previous_node = -1\n",
    "        current_node = self.root\n",
    "        last_visit = -1\n",
    "        current_prompt = self.stream\n",
    "        while current_node != -1 and self.nodes[current_node].end == False:\n",
    "            last_visit = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = last_visit\n",
    "            current_prompt += self.nodes[current_node].actions[last_visit]\n",
    "            previous_node = current_node\n",
    "            current_node = self.nodes[current_node].child[last_visit]\n",
    "\n",
    "        if current_node != -1:\n",
    "            return current_node, current_prompt\n",
    "        else:\n",
    "            if self.nodes[previous_node].actions[last_visit] == \"[answer]\\n\":\n",
    "                actions, probs = self.generate_answer_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "                self.nodes[current_node].end = True\n",
    "                for i in range(len(self.nodes[current_node].child)):\n",
    "                    self.nodes[current_node].mean_reward[i] = self.answer_evaluate(self.stream + self.nodes[current_node].actions[i] + '<|eot_id|>')\n",
    "            else:\n",
    "                actions, probs = self.generate_action_list(current_prompt) \n",
    "                self.nodes[previous_node].child[last_visit] = self.new_node(previous_node, (actions, probs))\n",
    "                current_node = self.nodes[previous_node].child[last_visit]\n",
    "            return current_node, current_prompt\n",
    "        \n",
    "    def simulation(self, current_node, current_prompt):\n",
    "        if self.nodes[current_node].end:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reward = self.nodes[current_node].mean_reward[optimum_child]\n",
    "            return reward\n",
    "        else:\n",
    "            optimum_child = self.nodes[current_node].get_optimum_child(self.policy_weight, self.explore_weight)\n",
    "            self.nodes[current_node].last_visit = optimum_child\n",
    "            reasoning = self.nodes[current_node].actions[optimum_child]\n",
    "            while(reasoning != \"[answer]\\n\"):\n",
    "                current_prompt += reasoning\n",
    "                reasoning = self.model(prompt = current_prompt, max_tokens = 256, temperature = 0, stop = '\\n', logprobs = 0)[\"choices\"][0][\"text\"] + \"\\n\"\n",
    "            current_prompt += reasoning\n",
    "            respond = self.model(prompt = current_prompt, max_tokens = 512, temperature = 0,  logprobs = 0)[\"choices\"][0][\"text\"] + \"<|eot_id|>\"\n",
    "            return self.answer_evaluate(self.stream + respond + '<|eot_id|>')\n",
    "            \n",
    "    def backpropagation(self, current_node, reward):\n",
    "        while current_node != -1:\n",
    "            self.nodes[current_node].visit_count += 1\n",
    "            self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit] += 1\n",
    "            self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit] += \\\n",
    "            (reward - self.nodes[current_node].mean_reward[self.nodes[current_node].last_visit]) / self.nodes[current_node].child_visit_count[self.nodes[current_node].last_visit]\n",
    "            current_node = self.nodes[current_node].parent\n",
    "\n",
    "    def query(self,query,iterations=100):\n",
    "        self.push_back(\"user\",query)\n",
    "        self.stream += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.MCTS_initialize()\n",
    "        while(not self.nodes[self.root].end):\n",
    "            while self.nodes[self.root].visit_count < iterations :\n",
    "                current_node, current_prompt = self.select_and_expand()\n",
    "                reward = self.simulation(current_node, current_prompt)\n",
    "                self.backpropagation(current_node, reward)\n",
    "            optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "            self.stream += self.nodes[self.root].actions[optimum_child]\n",
    "            for i in range(self.k):\n",
    "                if i != optimum_child and i != -1:\n",
    "                    self.delete_tree(self.nodes[self.root].child[i])\n",
    "            new_root = self.nodes[self.root].child[optimum_child]\n",
    "            self.delete_node(self.root)\n",
    "            self.set_root(new_root)\n",
    "        optimum_child = self.nodes[self.root].get_optimum_child(self.policy_weight,self.explore_weight)\n",
    "        respond = self.nodes[self.root].actions[optimum_child]\n",
    "        self.stream += respond + '<|eot_id|>'\n",
    "        return respond\n",
    "            \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b8120",
   "metadata": {},
   "source": [
    "# paragraph as node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] Starting MCTS with 20 iterations...\n",
      "[mcts_init] Initializing MCTS for query: What is the sum of the first 10 prime numbers?...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The answer is an outright refusal to help, which is unacceptable in an academic or prob...\n",
      "[self_evaluate] Sample 1 extracted score: -100\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The answer is an outright refusal to help, which is unacceptable in an academic or prob...\n",
      "[self_evaluate] Sample 1 extracted score: -100\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The answer provided is a single sentence stating that they cannot help with the questio...\n",
      "[self_evaluate] Sample 2 extracted score: -100\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The answer provided is a single sentence stating that they cannot help with the questio...\n",
      "[self_evaluate] Sample 2 extracted score: -100\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is an obvious refusal to attempt the problem, providing no explanation or ...\n",
      "[self_evaluate] Sample 3 extracted score: -95\n",
      "[self_evaluate] Final scores: [-100, -100, -95]\n",
      "[mcts_init] Root node created with Q-value: -99.16666666666666\n",
      "[run] Iteration 1/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is an obvious refusal to attempt the problem, providing no explanation or ...\n",
      "[self_evaluate] Sample 3 extracted score: -95\n",
      "[self_evaluate] Final scores: [-100, -100, -95]\n",
      "[mcts_init] Root node created with Q-value: -99.16666666666666\n",
      "[run] Iteration 1/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: Step 1:  First, we need to understand what prime numbers are. Prime numbers are numbers greater than...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: ### Step 1: Understand what prime numbers are.\n",
      "- **Good**: The answer correctly starts by defining p...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: ### Step 1: Understand what prime numbers are.\n",
      "- **Good**: The answer correctly starts by defining p...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is correct in identifying the first 10 prime numbers and the process to su...\n",
      "[self_evaluate] Sample 1 extracted score: 25\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is correct in identifying the first 10 prime numbers and the process to su...\n",
      "[self_evaluate] Sample 1 extracted score: 25\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is straightforward and correctly identifies the first 10 prime numbers. Ho...\n",
      "[self_evaluate] Sample 2 extracted score: 80\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is straightforward and correctly identifies the first 10 prime numbers. Ho...\n",
      "[self_evaluate] Sample 2 extracted score: 80\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear step-by-step process and justifica...\n",
      "[self_evaluate] Sample 3 extracted score: 35\n",
      "[self_evaluate] Final scores: [25, 80, 35]\n",
      "[run] Iteration 2/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear step-by-step process and justifica...\n",
      "[self_evaluate] Sample 3 extracted score: 35\n",
      "[self_evaluate] Final scores: [25, 80, 35]\n",
      "[run] Iteration 2/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] The sum of the first 10 prime numbers can be found by listing out the first 10 p...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] The sum of the first 10 prime numbers can be found by listing out the first 10 p...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: First, the student is correct that they are looking for the sum of the first 10 prime numbers. \n",
      "\n",
      "How...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: First, the student is correct that they are looking for the sum of the first 10 prime numbers. \n",
      "\n",
      "How...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it has some flaws. Firstly, the explanation of prim...\n",
      "[self_evaluate] Sample 1 extracted score: -30\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it has some flaws. Firstly, the explanation of prim...\n",
      "[self_evaluate] Sample 1 extracted score: -30\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but there are some minor flaws. The reasoning process i...\n",
      "[self_evaluate] Sample 2 extracted score: -20\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but there are some minor flaws. The reasoning process i...\n",
      "[self_evaluate] Sample 2 extracted score: -20\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear and explicit reasoning process. Th...\n",
      "[self_evaluate] Sample 3 extracted score: -50\n",
      "[self_evaluate] Final scores: [-30, -20, -50]\n",
      "[run] Iteration 3/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks a clear and explicit reasoning process. Th...\n",
      "[self_evaluate] Sample 3 extracted score: -50\n",
      "[self_evaluate] Final scores: [-30, -20, -50]\n",
      "[run] Iteration 3/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] First, we need to identify what prime numbers are. Prime numbers are numbers gre...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: [Reasoning process] First, we need to identify what prime numbers are. Prime numbers are numbers gre...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_critique] Critique generated successfully: The answer provided is a good attempt, but there are a few areas where it could be improved. Here's ...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[generate_critique] Critique generated successfully: The answer provided is a good attempt, but there are a few areas where it could be improved. Here's ...\n",
      "[self_evaluate] Evaluating solution with 3 samples...\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks proper reasoning and has minor flaws. The ...\n",
      "[self_evaluate] Sample 1 extracted score: -20\n",
      "[self_evaluate] Sample 1 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct, but it lacks proper reasoning and has minor flaws. The ...\n",
      "[self_evaluate] Sample 1 extracted score: -20\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct in identifying prime numbers and listing out the first 1...\n",
      "[self_evaluate] Sample 2 extracted score: -60\n",
      "[self_evaluate] Sample 2 response: \n",
      "\n",
      "[Analysis] The solution is mostly correct in identifying prime numbers and listing out the first 1...\n",
      "[self_evaluate] Sample 2 extracted score: -60\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly clear and easy to follow, but it lacks a thorough and rigorous a...\n",
      "[self_evaluate] Sample 3 extracted score: -70\n",
      "[self_evaluate] Final scores: [-20, -60, -70]\n",
      "[run] Iteration 4/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[self_evaluate] Sample 3 response: \n",
      "\n",
      "[Analysis] The solution is mostly clear and easy to follow, but it lacks a thorough and rigorous a...\n",
      "[self_evaluate] Sample 3 extracted score: -70\n",
      "[self_evaluate] Final scores: [-20, -60, -70]\n",
      "[run] Iteration 4/20\n",
      "[generate_refined_solution] Refining solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: First, we need to understand that prime numbers are numbers greater than 1 that have no divisors oth...\n",
      "[generate_critique] Generating critique for solution...\n",
      "[generate_refined_solution] Refined solution generated successfully: First, we need to understand that prime numbers are numbers greater than 1 that have no divisors oth...\n",
      "[generate_critique] Generating critique for solution...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MCTS_NODE:\n",
    "    def __init__(self, parent, solution, critique, Q_value):\n",
    "        self.parent = parent\n",
    "        self.solution = solution\n",
    "        self.critique = critique\n",
    "        self.Q_value = Q_value\n",
    "        self.visit_count = 0\n",
    "        self.children = []  \n",
    "        self.reward_samples = [] \n",
    "        self.fully_expanded = False \n",
    "        self.expanded_children = 0 \n",
    "    \n",
    "class MCTS_REASONING_LLM:\n",
    "    def __init__(self, model_path, max_child=5, c=1):\n",
    "        self.model = Llama(model_path, n_ctx=16384, n_gpu_layers=99, logits_all=False, verbose=False)\n",
    "        self.max_child = max_child\n",
    "        self.c = c\n",
    "        self.nodes = []\n",
    "        self.query = None\n",
    "        self.dummy_answers = [\n",
    "            \"I Don't Know\",\n",
    "            \"I can't understand this question.\",\n",
    "            \"I can't help with this question.\",\n",
    "            \"I don't know how to solve this question.\",\n",
    "            \"I don't know the answer to this question.\",\n",
    "            \"I don't know the answer to this question, sorry.\"\n",
    "        ]\n",
    "\n",
    "    def is_fully_expanded(self, idx): \n",
    "        if len(self.nodes[idx].children) >= self.max_child:\n",
    "            return True\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            if self.nodes[child_idx].Q_value > self.nodes[idx].Q_value:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_optimum_child(self, idx):\n",
    "        if not self.nodes[idx].children: \n",
    "            return -1\n",
    "        if self.nodes[idx].parent != -1:\n",
    "            parent_visit_count = self.nodes[self.nodes[idx].parent].visit_count\n",
    "        else:\n",
    "            parent_visit_count = 1\n",
    "        UCT = []\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            child_visit_count = self.nodes[child_idx].visit_count\n",
    "            uct_value = (self.nodes[child_idx].Q_value + \n",
    "                        self.c * math.sqrt(math.log(parent_visit_count + 1) / (child_visit_count + 1e-6)))\n",
    "            UCT.append(uct_value)\n",
    "        \n",
    "        if (not self.nodes[idx].fully_expanded and \n",
    "            np.max(UCT) < self.c * math.sqrt(math.log(parent_visit_count + 1) / 1e-6)):\n",
    "            return -1\n",
    "            \n",
    "        return self.nodes[idx].children[np.argmax(UCT)]\n",
    "\n",
    "    def generate_critique(self, query, solution):\n",
    "        print(f\"[generate_critique] Generating critique for solution...\")\n",
    "        \n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Since we have a weak Answer, could you provide me with a reflection or feedback to correct this answer better? Analyze this Answer Strictly and Critically, point out every flaw for every possible imperfect to minus every possible score!\n",
    "\n",
    "Question: {query}\n",
    "Answer: {solution}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=1024, temperature=0.8)\n",
    "            critique = response[\"choices\"][0][\"text\"].strip()\n",
    "            print(f\"[generate_critique] Critique generated successfully: {critique[:100]}...\")\n",
    "            return critique\n",
    "        except Exception as e:\n",
    "            print(f\"[generate_critique] ERROR: {str(e)}\")\n",
    "            return \"The answer needs improvement.\"\n",
    "\n",
    "    def generate_refined_solution(self, query, original_solution, critique):\n",
    "        print(f\"[generate_refined_solution] Refining solution...\")\n",
    "        \n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Please refine your answer according to the Reflection or Feedback. The response should begin with [reasoning process]...[Verification]... and end with \"[Final Answer] The answer is [answer formula]\"\n",
    "\n",
    "Question: {query}\n",
    "Original Answer: {original_solution}\n",
    "Feedback: {critique}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=2048, temperature=0.8)\n",
    "            refined = response[\"choices\"][0][\"text\"].strip()\n",
    "            print(f\"[generate_refined_solution] Refined solution generated successfully: {refined[:100]}...\")\n",
    "            return refined\n",
    "        except Exception as e:\n",
    "            print(f\"[generate_refined_solution] ERROR: {str(e)}\")\n",
    "            return original_solution\n",
    "\n",
    "    def extract_score_from_text(self, text):\n",
    "        # Look for patterns like [Score] -50, [Score]: -50, Score: -50, etc.\n",
    "        score_patterns = [\n",
    "            r'\\[Score\\]\\s*[-]?\\d+',  # [Score] -50\n",
    "            r'\\[Score\\]:\\s*[-]?\\d+',  # [Score]: -50\n",
    "            r'Score:\\s*[-]?\\d+',     # Score: -50\n",
    "            r'Score\\s+[-]?\\d+',      # Score -50\n",
    "            r'score\\s*[:=]\\s*[-]?\\d+',  # score: -50 or score = -50\n",
    "        ]\n",
    "        \n",
    "        for pattern in score_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                # Extract just the number from the match\n",
    "                number_match = re.search(r'[-]?\\d+', match.group())\n",
    "                if number_match:\n",
    "                    return int(number_match.group())\n",
    "        \n",
    "        # Fallback: look for the last number in the text (often the final score)\n",
    "        all_numbers = re.findall(r'[-]?\\d+', text)\n",
    "        if all_numbers:\n",
    "            # Filter numbers to reasonable score range\n",
    "            valid_scores = [int(num) for num in all_numbers if -100 <= int(num) <= 100]\n",
    "            if valid_scores:\n",
    "                return valid_scores[-1]  # Take the last valid score\n",
    "        \n",
    "        return 0  # Default if no score found\n",
    "\n",
    "    def self_evaluate(self, query, solution, num_samples=3):\n",
    "        print(f\"[self_evaluate] Evaluating solution with {num_samples} samples...\")\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(num_samples):\n",
    "            prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {query}\n",
    "Answer: {solution}\n",
    "\n",
    "Analyze this Answer Strictly and Critically, and point out every flaw for every possible imperfect to minus every possible score! You need to be very harsh and mean in calculating grades, and never give full marks to ensure that the marks are authoritative.\n",
    "\n",
    "Output a score between [-100,+100].\n",
    "\n",
    "Format: [Analysis] your analysis here [Score] your_number_here\n",
    "\n",
    "Example: [Analysis] The solution has calculation errors and lacks proper reasoning. [Score] -45<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.model(prompt=prompt, max_tokens=512, temperature=0.8)\n",
    "                text = response[\"choices\"][0][\"text\"]\n",
    "                print(f\"[self_evaluate] Sample {i+1} response: {text[:100]}...\")\n",
    "                \n",
    "                # Extract score using improved method\n",
    "                score = self.extract_score_from_text(text)\n",
    "                \n",
    "                # Full Score Suppression: reduce scores above 95\n",
    "                if score > 95:\n",
    "                    score = max(95, score - 10)\n",
    "                \n",
    "                # Clamp score to valid range\n",
    "                score = max(-100, min(100, score))\n",
    "                scores.append(score)\n",
    "                print(f\"[self_evaluate] Sample {i+1} extracted score: {score}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[self_evaluate] ERROR in sample {i+1}: {str(e)}\")\n",
    "                scores.append(0)\n",
    "        \n",
    "        print(f\"[self_evaluate] Final scores: {scores}\")\n",
    "        return scores\n",
    "    \n",
    "    def calculate_q_value(self, reward_samples):\n",
    "        \"\"\"Calculate Q value using formula from paper: Q(a) = 1/2 * (min(R_a) + mean(R_a))\"\"\"\n",
    "        if not reward_samples:\n",
    "            return 0\n",
    "        \n",
    "        min_reward = min(reward_samples)\n",
    "        mean_reward = sum(reward_samples) / len(reward_samples)\n",
    "        q_value = 0.5 * (min_reward + mean_reward)\n",
    "        \n",
    "        return q_value\n",
    "    \n",
    "    def update_q_value_with_children(self, node_index):\n",
    "        \"\"\"Update Q value considering children: Q'(a) = 1/2 * (Q(a) + max_child_Q)\"\"\"\n",
    "        node = self.nodes[node_index]\n",
    "        \n",
    "        # Calculate base Q value from own rewards\n",
    "        base_q = self.calculate_q_value(node.reward_samples)\n",
    "        \n",
    "        # Find maximum Q value among children\n",
    "        max_child_q = float('-inf')\n",
    "        has_children = False\n",
    "        \n",
    "        for child_idx in node.children:\n",
    "            child_q = self.nodes[child_idx].Q_value\n",
    "            max_child_q = max(max_child_q, child_q)\n",
    "            has_children = True\n",
    "        \n",
    "        # Update Q value: Q'(a) = 1/2 * (Q(a) + max_child_Q)\n",
    "        if has_children:\n",
    "            node.Q_value = 0.5 * (base_q + max_child_q)\n",
    "        else:\n",
    "            node.Q_value = base_q\n",
    "\n",
    "    def mcts_init(self, query):\n",
    "        print(f\"[mcts_init] Initializing MCTS for query: {query[:50]}...\")\n",
    "        \n",
    "        self.query = query\n",
    "        self.nodes = []\n",
    "        \n",
    "        # Create root node with dummy answer\n",
    "        dummy_solution = random.choice(self.dummy_answers)\n",
    "        critique = self.generate_critique(self.query, dummy_solution)\n",
    "        \n",
    "        # Create root node\n",
    "        root_node = MCTS_NODE(-1, dummy_solution, critique, 0)\n",
    "        \n",
    "        # Evaluate root node\n",
    "        root_node.reward_samples = self.self_evaluate(query, dummy_solution)\n",
    "        root_node.Q_value = self.calculate_q_value(root_node.reward_samples)\n",
    "        root_node.visit_count = 1\n",
    "        \n",
    "        self.nodes.append(root_node)\n",
    "        print(f\"[mcts_init] Root node created with Q-value: {root_node.Q_value}\")\n",
    "\n",
    "    def iterator(self):\n",
    "        \"\"\"Single MCTS iteration combining all phases: Selection -> Expansion -> Evaluation -> Backpropagation\"\"\"\n",
    "        # SELECTION PHASE: Navigate to leaf node\n",
    "        current_node = 0\n",
    "        previous_node = -1\n",
    "        while current_node != -1:\n",
    "            previous_node = current_node\n",
    "            current_node = self.get_optimum_child(current_node)\n",
    "        \n",
    "        # EXPANSION PHASE: Create refined solution\n",
    "        solution = self.generate_refined_solution(\n",
    "            self.query, \n",
    "            self.nodes[previous_node].solution, \n",
    "            self.nodes[previous_node].critique \n",
    "        )\n",
    "        critique = self.generate_critique(self.query, solution)\n",
    "        \n",
    "        # Create new child node\n",
    "        new_node = MCTS_NODE(previous_node, solution, critique, 0)\n",
    "        self.nodes.append(new_node)\n",
    "        current_node = len(self.nodes) - 1\n",
    "        \n",
    "        # Add child to parent's children list\n",
    "        self.nodes[previous_node].children.append(current_node)\n",
    "        \n",
    "        # EVALUATION PHASE: Self-evaluate the new solution\n",
    "        self.nodes[current_node].reward_samples = self.self_evaluate(self.query, solution)\n",
    "        self.nodes[current_node].Q_value = self.calculate_q_value(self.nodes[current_node].reward_samples)\n",
    "        self.nodes[current_node].visit_count = 1\n",
    "        \n",
    "        # BACKPROPAGATION PHASE: Update Q values up the tree\n",
    "        while previous_node != -1:\n",
    "            self.nodes[previous_node].visit_count += 1\n",
    "            self.update_q_value_with_children(previous_node)\n",
    "            self.nodes[previous_node].fully_expanded = self.is_fully_expanded(previous_node)  # Fixed: was 'fully_expended'\n",
    "            previous_node = self.nodes[previous_node].parent\n",
    "\n",
    "    def run(self, query, iterations=100):\n",
    "        \"\"\"Run MCTS for specified number of iterations\"\"\"\n",
    "        print(f\"[run] Starting MCTS with {iterations} iterations...\")\n",
    "        self.mcts_init(query)\n",
    "        for i in range(iterations):\n",
    "            print(f\"[run] Iteration {i+1}/{iterations}\")\n",
    "            self.iterator()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "                print(f\"[run] Best Q-value after {i+1} iterations: {best_node.Q_value}\")\n",
    "        \n",
    "        # Return best solution\n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        print(f\"[run] Final best Q-value: {best_node.Q_value}\")\n",
    "        return best_node.solution\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"Get the solution with highest Q-value\"\"\"\n",
    "        if not self.nodes:\n",
    "            return None\n",
    "        \n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        return best_node.solution\n",
    "    \n",
    "\n",
    "# Initialize the model\n",
    "mctsr = MCTS_REASONING_LLM(\"llama-3.1-8b-instruct-q4_k_m.gguf\")\n",
    "\n",
    "# Solve a problem\n",
    "query = \"What is the sum of the first 10 prime numbers?\"\n",
    "solution = mctsr.run(query, 20)\n",
    "\n",
    "# Get tree statistics\n",
    "stats = mctsr.get_tree_stats()\n",
    "print(f\"Generated {stats['total_nodes']} nodes with max Q-value: {stats['max_q_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6971b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class MCTS_NODE:\n",
    "    def __init__(self, parent, solution, critique, Q_value):\n",
    "        self.parent = parent\n",
    "        self.solution = solution\n",
    "        self.critique = critique\n",
    "        self.Q_value = Q_value\n",
    "        self.visit_count = 0\n",
    "        self.children = []\n",
    "        self.reward_samples = []\n",
    "        self.fully_expanded = False\n",
    "\n",
    "class MCTS_REASONING_LLM:\n",
    "    def __init__(self, model_path, max_child=5, c=1):\n",
    "        self.model = Llama(model_path, n_ctx=16384, n_gpu_layers=99, logits_all=False, verbose=False)\n",
    "        self.max_child = max_child\n",
    "        self.c = c\n",
    "        self.nodes = []\n",
    "        self.query = None\n",
    "        self.dummy_answers = [\n",
    "            \"I Don't Know\",\n",
    "            \"I can't understand this question.\",\n",
    "            \"I can't help with this question.\",\n",
    "            \"I don't know how to solve this question.\",\n",
    "            \"I don't know the answer to this question.\",\n",
    "            \"I don't know the answer to this question, sorry.\"\n",
    "        ]\n",
    "\n",
    "    def is_fully_expanded(self, idx):\n",
    "        if len(self.nodes[idx].children) >= self.max_child:\n",
    "            return True\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            if self.nodes[child_idx].Q_value > self.nodes[idx].Q_value:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_optimum_child(self, idx):\n",
    "        if not self.nodes[idx].children:\n",
    "            return -1\n",
    "        parent_visit_count = self.nodes[self.nodes[idx].parent].visit_count if self.nodes[idx].parent != -1 else 1\n",
    "        UCT = []\n",
    "        for child_idx in self.nodes[idx].children:\n",
    "            child_visit_count = self.nodes[child_idx].visit_count\n",
    "            uct_value = (self.nodes[child_idx].Q_value +\n",
    "                         self.c * math.sqrt(math.log(parent_visit_count + 1) / (child_visit_count + 1e-6)))\n",
    "            UCT.append(uct_value)\n",
    "        if not self.nodes[idx].fully_expanded and np.max(UCT) < self.c * math.sqrt(math.log(parent_visit_count + 1) / 1e-6):\n",
    "            return -1\n",
    "        return self.nodes[idx].children[np.argmax(UCT)]\n",
    "\n",
    "    def generate_critique(self, query, solution):\n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Since we have a weak Answer, could you provide me with a reflection or feedback to correct this answer better? Analyze this Answer Strictly and Critically, point out every flaw for every possible imperfect to minus every possible score!\n",
    "\n",
    "Question: {query}\n",
    "Answer: {solution}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=1024, temperature=0.8)\n",
    "            return response[\"choices\"][0][\"text\"].strip()\n",
    "        except:\n",
    "            return \"The answer needs improvement.\"\n",
    "\n",
    "    def generate_refined_solution(self, query, original_solution, critique):\n",
    "        prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Please refine your answer according to the Reflection or Feedback. The response should begin with [reasoning process]...[Verification]... and end with \"[Final Answer] The answer is [answer formula]\"\n",
    "\n",
    "Question: {query}\n",
    "Original Answer: {original_solution}\n",
    "Feedback: {critique}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Let's think step by step.\"\"\"\n",
    "        try:\n",
    "            response = self.model(prompt=prompt, max_tokens=2048, temperature=0.8)\n",
    "            return response[\"choices\"][0][\"text\"].strip()\n",
    "        except:\n",
    "            return original_solution\n",
    "\n",
    "    def extract_score_from_text(self, text):\n",
    "        score_patterns = [\n",
    "            r'\\[Score\\]\\s*[-]?\\d+',\n",
    "            r'\\[Score\\]:\\s*[-]?\\d+',\n",
    "            r'Score:\\s*[-]?\\d+',\n",
    "            r'Score\\s+[-]?\\d+',\n",
    "            r'score\\s*[:=]\\s*[-]?\\d+',\n",
    "        ]\n",
    "        for pattern in score_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                number_match = re.search(r'[-]?\\d+', match.group())\n",
    "                if number_match:\n",
    "                    return int(number_match.group())\n",
    "        all_numbers = re.findall(r'[-]?\\d+', text)\n",
    "        if all_numbers:\n",
    "            valid_scores = [int(num) for num in all_numbers if -100 <= int(num) <= 100]\n",
    "            if valid_scores:\n",
    "                return valid_scores[-1]\n",
    "        return 0\n",
    "\n",
    "    def self_evaluate(self, query, solution, num_samples=3):\n",
    "        scores = []\n",
    "        for _ in range(num_samples):\n",
    "            prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {query}\n",
    "Answer: {solution}\n",
    "\n",
    "Analyze this Answer Strictly and Critically, and point out every flaw for every possible imperfect to minus every possible score! You need to be very harsh and mean in calculating grades, and never give full marks to ensure that the marks are authoritative.\n",
    "\n",
    "Output a score between [-100,+100].\n",
    "\n",
    "Format: [Analysis] your analysis here [Score] your_number_here\n",
    "\n",
    "Example: [Analysis] The solution has calculation errors and lacks proper reasoning. [Score] -45<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "            response = self.model(prompt=prompt, max_tokens=512, temperature=0.8)\n",
    "            text = response[\"choices\"][0][\"text\"]\n",
    "            score = self.extract_score_from_text(text)\n",
    "            if score > 95:\n",
    "                score = max(95, score - 10)\n",
    "            score = max(-100, min(100, score))\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "    def calculate_q_value(self, reward_samples):\n",
    "        if not reward_samples:\n",
    "            return 0\n",
    "        min_reward = min(reward_samples)\n",
    "        mean_reward = sum(reward_samples) / len(reward_samples)\n",
    "        return 0.5 * (min_reward + mean_reward)\n",
    "\n",
    "    def update_q_value_with_children(self, node_index):\n",
    "        node = self.nodes[node_index]\n",
    "        base_q = self.calculate_q_value(node.reward_samples)\n",
    "        max_child_q = float('-inf')\n",
    "        has_children = False\n",
    "        for child_idx in node.children:\n",
    "            child_q = self.nodes[child_idx].Q_value\n",
    "            max_child_q = max(max_child_q, child_q)\n",
    "            has_children = True\n",
    "        if has_children:\n",
    "            node.Q_value = 0.5 * (base_q + max_child_q)\n",
    "        else:\n",
    "            node.Q_value = base_q\n",
    "\n",
    "    def mcts_init(self, query):\n",
    "        self.query = query\n",
    "        self.nodes = []\n",
    "        dummy_solution = random.choice(self.dummy_answers)\n",
    "        critique = self.generate_critique(self.query, dummy_solution)\n",
    "        root_node = MCTS_NODE(-1, dummy_solution, critique, 0)\n",
    "        root_node.reward_samples = self.self_evaluate(query, dummy_solution)\n",
    "        root_node.Q_value = self.calculate_q_value(root_node.reward_samples)\n",
    "        root_node.visit_count = 1\n",
    "        self.nodes.append(root_node)\n",
    "\n",
    "    def iterator(self):\n",
    "        current_node = 0\n",
    "        previous_node = -1\n",
    "        while current_node != -1:\n",
    "            previous_node = current_node\n",
    "            current_node = self.get_optimum_child(current_node)\n",
    "        solution = self.generate_refined_solution(\n",
    "            self.query,\n",
    "            self.nodes[previous_node].solution,\n",
    "            self.nodes[previous_node].critique\n",
    "        )\n",
    "        critique = self.generate_critique(self.query, solution)\n",
    "        new_node = MCTS_NODE(previous_node, solution, critique, 0)\n",
    "        self.nodes.append(new_node)\n",
    "        current_node = len(self.nodes) - 1\n",
    "        self.nodes[previous_node].children.append(current_node)\n",
    "        self.nodes[current_node].reward_samples = self.self_evaluate(self.query, solution)\n",
    "        self.nodes[current_node].Q_value = self.calculate_q_value(self.nodes[current_node].reward_samples)\n",
    "        self.nodes[current_node].visit_count = 1\n",
    "        while previous_node != -1:\n",
    "            self.nodes[previous_node].visit_count += 1\n",
    "            self.update_q_value_with_children(previous_node)\n",
    "            self.nodes[previous_node].fully_expanded = self.is_fully_expanded(previous_node)\n",
    "            previous_node = self.nodes[previous_node].parent\n",
    "\n",
    "    def run(self, query, iterations=5):\n",
    "        self.mcts_init(query)\n",
    "        for _ in range(iterations):\n",
    "            self.iterator()\n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        return best_node.solution\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        if not self.nodes:\n",
    "            return None\n",
    "        best_node = max(self.nodes, key=lambda n: n.Q_value)\n",
    "        return best_node.solution\n",
    "    \n",
    "    def print_status(self):\n",
    "        print(\"printing MCTS status\")\n",
    "        idx = 0\n",
    "        for nodes in self.nodes:\n",
    "            print(f\"node_{idx}:\\n\"\n",
    "                  f\"parent: {nodes.parent}, children: {nodes.children}\\n\"\n",
    "                  f\"solution:\\n\"\n",
    "                  f\"{nodes.solution}\\n\"\n",
    "                  \"critique:\\n\"\n",
    "                  f\"{nodes.critique}\\n\"\n",
    "                  f\"Q_value: {nodes.Q_value}\\n\"\n",
    "                  f\"evaluate_samples:\\n\"\n",
    "                  f\"{nodes.reward_samples}\\n\")\n",
    "            idx += 1\n",
    "\n",
    "# Function to generate direct answer using Llama\n",
    "def generate_direct_answer(model, query):\n",
    "    prompt = f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {query}\n",
    "Please provide the answer directly without any reasoning or explanation.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    try:\n",
    "        response = model(prompt=prompt, max_tokens=512, temperature=0.8)\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except:\n",
    "        return \"Unable to generate answer.\"\n",
    "\n",
    "# Load riddle data\n",
    "df = pd.read_excel(\"Riddle.xlsx\")\n",
    "\n",
    "# Initialize Llama model (replace with actual model path)\n",
    "mcts = MCTS_REASONING_LLM(model_path=\"llama-3.1-8b-instruct-q4_k_m.gguf\")\n",
    "llm = mcts.model\n",
    "# Test on a subset of riddles (e.g., first 5 for demonstration)\n",
    "results = []\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    query = row[\"Question\"]\n",
    "    actual_answer = row[\"Answer\"]\n",
    "    \n",
    "    # Generate MCTS answer\n",
    "    mcts_answer = mcts.run(query, iterations=5)\n",
    "    \n",
    "    # Generate direct answer\n",
    "    direct_answer = generate_direct_answer(llm, query)\n",
    "\n",
    "    mcts.print_status()\n",
    "\n",
    "    print(\"\\nComparison of MCTS, Direct, and Actual Answers:\\n\")\n",
    "    print(f\"Question: {query}\\n\"\n",
    "          f\"Actual_answer: {actual_answer}\\n\"\n",
    "          f\"MCTS_answer: {mcts_answer}\\n\"\n",
    "          f\"direct_answer: {direct_answer}\\n\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"Question\": query,\n",
    "        \"Actual Answer\": actual_answer,\n",
    "        \"MCTS Answer\": mcts_answer,\n",
    "        \"Direct Answer\": direct_answer\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"riddle_comparison.csv\", index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nComparison of MCTS, Direct, and Actual Answers:\")\n",
    "print(results_df[[\"ID\", \"Question\", \"Actual Answer\", \"MCTS Answer\", \"Direct Answer\"]].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
